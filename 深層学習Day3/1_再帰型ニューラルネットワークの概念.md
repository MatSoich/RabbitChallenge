<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

# 確認テスト１９
- サイズ 5*5の入力画像を、サイズ3*3のフィルタで畳み込んだ時の出力画像のサイズを求めよ。なおストライド2パディングは1とする。
- 正解は3*3。

# 再帰型ニューラルネットワーク(RNN)
- 前後のデータに関係性がある、時系列データ・自然言語データなどに使用される。
- RNNでは入力層から中間層（W_in）、中間層から出力層(W_out)、１つ前の学習での中間層から中間層（W）という３つの重みがあり、それをうまく学習することが重要である。
![kakunin](imgs/RNN.png)
# 確認テスト２０
- RNNのネットワークには大きく分けて３つの重みがある。
  - １つは入力から現在の中間層を定義する際にかけられて重み、
  - １つは中間層から出力を定義する際にかけられた重み、
- 残り１つの重みについて説明せよ。
  - 残り１つは１つ前の中間層から中間層を定義する際にかけられた重み。

# RNN続き
- プログラムは3_1参照
- バイナリ加算
  - 過去から未来への情報の遷移を繰り上がりによって表現している。
- 時系列ループ
  - RNNでは1回の学習で、分析機関の学習を一旦全て死してしまう。それを学習の数繰り返すことで、モデルの制度を高めていく。（一回の学習で必要な1年のモデルなら1年分の予測をす全てしてしまう。）
# 演習チャレンジ
![kakunin](imgs/EnshuChallange03.png)
- 構文器の問題
  - 隣同士の単語をつなぎ合わせて行って、一つの特徴量にしていく木構造のプロセスを構文器という。
![kakunin](imgs/kobunki1.png)
![kakunin](imgs/kobunki2.png)
- 演習の正解は2
- 重みをうまく準備することによって、特徴量を構成する数字の数は調整できる。

# 確認テスト２１（誤差逆伝播法の復習）
> \\\(\begin{aligned}z &= t^2\cr
t &= x + y\cr
\displaystyle \frac{dz}{dt} &= 2t\cr
\displaystyle \frac{dt}{dx} &= 1\end{aligned}\\\)
> 連鎖率の原理より<br>
> \\\(\displaystyle \frac{dz}{dx} = \frac{dz}{dt}\frac{dt}{dx}\\\)<br>
> \\\(\displaystyle \frac{dz}{dx} = 2t \cdot 1 = 2(x + y)\\\)<br>

# RNNにおける逆伝播(BPTT)
- 3_1_Simple_RNNの当該部分
```python
delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])
```
- deltaの定義(E（もしくはy）をuまで順次微分したよというのを表す。)
```python
delta = np.zeros((hidden_layer_size, binary_dim + 1))
```
# BPTTの数学的説明１
>\\\(\begin{aligned}
\frac{\partial E}{\partial W_{(in)}} &= \frac{\partial E}{\partial u^t} \left[ \frac{\partial u^t}{\partial W_{(in)}} \right]^T = \delta^t\left[ x^t \right]^T \cr
\frac{\partial E}{\partial W_{(out)}} &= \frac{\partial E}{\partial v^t} \left[ \frac{\partial v^t}{\partial W_{(out)}} \right]^T = \delta^{out,t}\left[ z^t \right]^T \cr
\frac{\partial E}{\partial W} &= \frac{\partial E}{\partial u^t} \left[ \frac{\partial u^t}{\partial W} \right]^T = \delta^t\left[ z^{t-1} \right]^T \cr
\frac{\partial E}{\partial b} &= \frac{\partial E}{\partial u^t}  \frac{\partial u^t}{\partial b} = \delta^t \cr
\frac{\partial E}{\partial c} &= \frac{\partial E}{\partial v^t}  \frac{\partial v^t}{\partial c} = \delta^{out,t}
\end{aligned}\\\)
# BPTTの数学的説明2
活性化関数f,gとして、
> \\\(\begin{aligned}
u_t &= W_{(in)}x^t + Wz^{t-1} + b \cr
z_t &= f \left( W_{(in)}x^t + Wz^{t-1} + b \right) \cr
v_t &= W_{(out)}z^t + c \cr
y_t &=g \left( W_{(out)}z^t + c\right) \end{aligned}\\\)
# 確認テスト２２
- 下図の\\\(y_1\\\)を\\\(x \cdot z_0 \cdot z_1 \cdot W_{in} \cdot W \cdot W_{out}\\\)を用いて数式で表せ。
- ※また中間層の出力にシグモイド関数g(x)を作用させよ。
![kakunin](imgs/RNN.png)
> \\\(\begin{}上図より、適当なバイアスz_1を置いて、\cr
y
\\\)
# BPTTの数学的説明3(deltaの定義の説明)
- 1つめの式（delta定義そのもの）
- ２つ目と３つ目の式、delta_t とdelta_{t-1}の時間的なつながり
- 
# BPTTの数学的説明4(重みとバイアスの更新式)
- ２番目の式は中間層から先のプロセスなので、時間的に前の式を考慮しなくて良いものになっている。（sumの表現がそこだけない。）
- εは学習率（ハイパーパラメータなので、時間的に本質ではない。）

# BPTTの全体像
- どうしてBPTTで時間的な考慮ができるのか数式を通じて確認できる。
- T時刻の誤差関数クロスエントロピー
- 誤差関数はニューラルネットワークの出力であるYと教師データであるEから構成される
- 


