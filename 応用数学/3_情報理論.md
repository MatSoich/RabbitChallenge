応用数学
============
# 情報理論

- 情報科学
  - 情報量が１増える場合でも、元々の量が違うと見分けやすいか見分けにくいか違う。
  - それを数量的に判断するのが情報量
  - 具体的には増加の比率として表す。
  - 100gが110gになるときづく人は1000gが1010gになると気づくかというとそうではない。比率の場合が多く、1100gとかにしないとわからないパターンが多い。
  - 自己情報量
    - 数が１ずつ増えているときに全体に対する増分の比率は 全体をW(事象の総数)として1/Wそれを連続変数と考えて積分すると、logWとなる。これが情報量を表す単位となる。
    - I(x) = ー log(P(x)) = log（(W(x))
    - P(確率)はW(事象の総数)の逆数になるので、logをとると-が表に出て来る。
    - 対数の底が2の時は、単位はbit
    - 自然対数の時は単位はnat
    - スイッチの数で考えると、２でログを取るという発想になる。（と考えることもできる。）
    - 確率は珍しいほど確率が低いので、逆数をとって、ログを取っているのっで、結果マイナスにすると、珍しいほど情報量が大きくなるという直感に即した結果になる。
  - シャノンエントロピー・・・自己情報量の期待値。
    - \(H(X) = E(I(X)) = - E(log(P(x))) = -\displaystyle \sum\ (P(x) * log(P(x)))\)
    - シャノンエントロピーは確率x=0.5の時に最大になる傾向。逆に0 or 1になるとつまり決まった結果が出るということで、新たな情報量が少ないので、新しく得られる情報は少ないので、シャノンエントロピーの値は0になる。
  - カルバックライブラーダイバージェンス
    - 元々考えられていた分布（Q）が実際測ってみたら（P）だったというときにその違いはどれくらいかというときに使うのが、カルバックライブラーダイバージェンスという。
    - Pという確率分布とqという確率分布の性質の違いを距離のようなものを表していると考えられる。性質は距離に近い。距離の定義をすべて満たしているわけではない。(逆から測ったら同じにならない)
    - \(D_KL(P||Q) = E_x~P[log(P(x) / Q(x))] =　E_x~P[log(P(x)) - log(Q(x))] = E_x~P[I(Q(x)) - I(P(x))]\)
    - 最初に見積もった情報量と後からわかった情報量の差の期待値(新しい分布で情報は比較される)
    - カルバックライブラーダイバージェンスは情報利得と捉える考え方もできる。
    - \(D_KL(P||Q) = E_x~P[log(P(x)/Q(x))] = Σ(P(x) * log(P(x)/Q(x))\)
    - シャノンエントロピーと形が似ている。
      - 関係している。
  - 交差エントロピー
    - KLダイバージェンスの一部分を取り出したもの
    - 想定していた分布Qについての自己情報量I(Q)を分布Pで平均をとる。
    - D_KL(P||Q) = Σ(P(x) * ((-log(Q(x))) - (-log(P(x))))のΣ(P(x) * -log(Q(x)))の部分が交差エントロピーH(P,Q)で表す
    - Σ(P(x) * -log(P(x)))をH(P)として、
    - H(P,Q) = H(P) + D_KL(P||Q)
    - また
    - H(P,Q) = - E_x~P[ I(Q(x)) ]