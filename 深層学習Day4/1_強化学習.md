<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

# 強化学習とは
- 強化学習の応用例
  - キャンペーンの案内
    - コストを小さくしつつ、売り上げを最大化する。
    - 不完全な知識を元にしながらも行動してトライアル＆エラーして最適な行動を見つけていく
    - 強化学習は方策価値関数と行動価値関数を両方学習する。
      - 現在人間が実施している作業である、法則やルールを発見していく作業ができる。

- 強化学習と教師あり学習の違い。
  - 強化学習は方策（ルール）を見つける動き
  - 深層学習は特徴量を見つける動き

- 強化学習のアイデアは昔からあったが、コンピュータの計算速度が向上したことで実現可能になってきた。
  - Q学習と関数近似法を組み合わせる手法の登場（近年の理論的に重要）。
    - Q学習
      - 行動価値関数を使う学習の方法
    - 関数近似法
      - ルールベースを一部使っていた部分を関数に置き換えた物（つまりニューラルネットワーク）にした。


# 価値関数
- 状態価値関数と行動価値関数
  - 状態価値観数
    - 環境の状態が良ければ価値が上がる。エージェントの行動は関係ない。
  - 行動価値関数
    - 環境を元にして、エージェントの行動を元にして価値を出す。
- アルファ碁などでは行動価値関数が使われている。
# 方策関数
- ある状態でどのような行動をとるのか（その確率を）決める関数
  - \\\(π(s)=a\\\)

関数の関係
- \\\(π(s,a) \cdots\\\) VやQを元にどのように行動するか。
  - その瞬間その瞬間の行動を決定する関数
- \\\(V(s) \cdots\\\) 状態関数
- \\\(Q(s,a) \cdots\\\)状態 + 行動関数
  - ゴールまで今の方策を続けた時の報酬の予測値が得られる
    - やり続けたら最終的にどうなるか

- 強化学習は価値関数と方策関数の２つが優秀ならうまくいく
  - 将来のことを考えながら、今の行動を選べる人


# 方策反復法
- 方策をモデル化(NN化)して学習する方法
  - 方策勾配法\\\(π(s,a|θ）\\\)
    - \\\(θ_{t+1} =θ_t + \theta \nabla_\theta J(θ)\\\)
      - \\\(\pi \\\)を学習するための式。学習を進めたいので、プラスが使われている。
    - 平均報酬と割引報酬和の定義に対応して、行動価値関数Qの定義を行い、以下のような方策勾配定理が成り立つ。
    - \\\(\displaystyle \begin{aligned} \nabla_\theta J(θ) &=\nabla_\theta \sum_{a \in A} \pi_\theta (a|s) Q_\pi(s,a) \cr &= E_{\pi_\theta}\left[\nabla_\theta log \pi_\theta (a|s) Q_\pi(s,a) \right] \end{aligned}\\\)
    - J(θ)はNNでは誤差関数だったが、ここでは機体収益を表す（NNでは誤差を小さく、ここでは機体収益を大きくしないといけない）
J 方策の良さ
θ 方策関数のパラメータ



