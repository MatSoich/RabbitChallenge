強化学習と教師あり学習の違い。
強化学習は方策（ルール）を見つける動き
深層学習は特徴量を見つける動き

Q学習
行動価値関数を使う学習の方法

と

関数近似法
ルールベースを一部使っていた部分を関数に置き換えた物（つまりニューラルネットワーク）にした。
価値関数や方策関数を分析している。


価値関数と方策関数

価値関数
状態価値関数と行動価値関数
状態価値観数

方策関数
ある状態でどのような行動をとるのか（その確率を）決める関数
π(s)=a

関数の関係
π(s,a) VやQを元にどのように行動するか。その瞬間その瞬間の行動を決定する関数
V(s) 状態関数
Q(s,a)状態＋行動関数
ゴールまで今の方策を続けた時の報酬の予測値が得られる。
→やり続けたら最終的にどうなるか


強化学習は価値関数と方策関数の２つが優秀ならうまくいく
→将来のことを考えながら、今の行動を選べる人

関数はニューラルネットワークにできる
→学習できる
→方策関数をNNとして学習させる

方策反復法
方策をモデル化して学習する方法
方策勾配法（π(s,a|θ）
θ_t+1 =θ_t + εΔJ(θ)
J(θ)はNNでは誤差関数だったが、ここでは機体収益を表す（NNでは誤差を小さく、ここでは機体収益を大きくしないといけない）
J 方策の良さ
θ 方策関数のパラメータ


Alpha GO

Alpha Go Lee
Alpha Go zero


Alpha Go lee
Value NetとPolicy Net


入力が２次元の場合、Convolution
出力が２次元ならSoftmaxなどで最終的に整形
出力が１次元なら全結合層を使う。
