<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


3 軽量化・高速化技術
==========
# 軽量化・高速化技術
- 分散深層学習
  - 高速な計算が求められる深層学習において、複数の計算資源を利用した並列処理は重要
  - 主に以下の３つ
    - データ並列化
    - モデル並列化
    - GPU
  - マシンの性能によらず処理を進めるためのメモリや計算量の節約
  - 主に以下の３つ
    - 量子化
    - 蒸留
    - プルーニング
## データ並列
- 同期型
  - 別々のデータを使って並列で処理した結果を一回集めて平均を取って、親のモデルの更新を行い、その更新済みモデルを使って学習を各子ノードが再開する。
- 非同期型
  - 終わった順に親のパラメータを更新してその親のパラメータを持ってきて、各ノードは次の学習を即座に開始する。
  - 非同期型は最新のモデルのパラメータを利用できないので学習が不安定になりやすい（Stale Gradient Problem）
- 現在は同期型が主流（精度が非同期型が多いため）
  - ただ分散の方法によっては非同期型にすることもある。（世界中の提供されたパソコンを使って学習させるようなプロジェクトの場合）

## モデル並列
- NNのモデルを縦に区切った理、分岐しているNNの場合は横方向に分割して、各部分について、子ノードに学習させる方法。
- 実際には枝分かれ部分を分割して学習させることが多い。
- モデル並列化は１台のパソコンで複数のGPUに作業を分割させることが多い。
  - 最終的な出力を元にパラメータの更新を行うので、１台のパソコンにまとめた方が効率が良い。
- 当然だが、大きなモデルでないと効果は薄い。
## GPU
- CPU
  - 高性能なコアが少数
  - 性能向上が計算量の需要に追いついていない
- GPU
  - 低性能なコア（計算できる種類が少ないコア）が多数
  - NNと非常に相性が良い。（NNは行列演算、つまり四則演算ができれば良いから）
- GPGPU
  - 元々GPUはゲーム用途などグラフィック処理のために用いられていたが、NNと非常に相性が良いということで用途をより一般的にしたGeneral Purpose GPUというものが出て狂うようになった。
- GPGPU開発環境
  - CUDA
    - NVIDIA社開発GPUのみで使用可能。
    - DeepLearning用に提供されているので使いやすい。DeepLearning用GPGPU環境のデファクトスタンダード
  - OpenCL
    - NVIDIA以外のGPUメーカー（Intel, AMD, ARM）などで使用できる。
    - Deep Learning用としてはほぼ使われない。
  - CUDA利用の際はTensorflowやPyTorchで利用できるような実装がなされているので、、GPUを指定して使用できるようにしておけば意識せず使える。
## 量子化（Quantization）
- 64bit →  32bitなどパラメータの不動小数点精度を下位のものに移す。（省力化）
  - 基本的に16bit（半精度） or 32bit（単精度） or 64bit（倍精度）
  - bit数を少なくすると少数の精度が悪くなる。
  - 計算は当然量子化が少ない単位の方が高速化する。
- 省力化・高速化　←→ 精度　のトレードオフ
- 機械学習は16bitで良い場合は多い。
##　蒸留
- 精度の高い大きなモデルの知見やネットワークから軽量なモデルを作成する。
- 教師モデル（大きいモデル）と生徒モデル（程々の良い性能で軽い）
- 蒸留の性能
- 150層から11層にしてもある程度の性能が出ることがcifar10データを使った実験で出ている。
  - Adriana et all. (2015) - "FITNETS: HINTS FOR THIN DEEP NETS"
## プルーニング
- 役に立たないパラメータを削除する。
  - 大きいNNでも使われないパラメータが出ることがわかっている。
- やり方としては、重みの小さいニューロンを削除し、際学習を行う。
- 割と消せる上に意外と性能が変わらない。
  - 佐々木 et al. 「ニューラルネットワークの全結合層におけるパラメータ削減手法の比較」
    - ５割削除しても元のネットワークの91%、94%削除しても、元のネットワークの90％の精度

# 実装

- pytorchの場合、以下のようにdataloaderの"num_workers"変数をコントロールすることでモデルの並列化を任意に実現できる。

```python
# デフォルト設定のDataLoaderの場合
train_loader_default = torch.utils.data.DataLoader(dataset1,batch_size=mini_batch_size)
test_loader_default = torch.utils.data.DataLoader(dataset2,batch_size=mini_batch_size)

# データローダー：2
train_loader_nworker = torch.utils.data.DataLoader(
    dataset1, batch_size=mini_batch_size, num_workers=2)
test_loader_nworker = torch.utils.data.DataLoader(
    dataset2, batch_size=mini_batch_size, num_workers=2)


# データローダー：フル
train_loader_nworker = torch.utils.data.DataLoader(
    dataset1, batch_size=mini_batch_size, num_workers=os.cpu_count())
test_loader_nworker = torch.utils.data.DataLoader(
    dataset2, batch_size=mini_batch_size, num_workers=os.cpu_count())

```
