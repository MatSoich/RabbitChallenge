<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

# A3C
- Asynchronous Advantage Actor-Critic（A3C）とは•強化学習の学習法の一つ
  - DeepMindのVolodymyr Mnih(ムニ)のチームが提案
  - 特徴：複数のエージェントが同一の環境で非同期に学習すること
- A3Cによる非同期(Asynchronous) 学習の詳細
  - 複数のエージェントが並列に自律的に、rollout (ゲームプレイ) を実行し、勾配計算を行う
  - 各エージェントは好き勝手なタイミングで共有ネットワークを更新する
  - 各エージェントは定期的に自分のネットワーク(local network) の重みをglobal networkの重みと同期する
  - 共有ネットワーク= パラメータサーバ
- 並列分散エージェントで学習を行うA3Cのメリット
  - ①学習が高速化
  - ②学習を安定化
    - ②について：
      - 経験の自己相関が引き起こす学習の不安定化は、強化学習の長年の課題
      - DQNはExperience Replay (経験再生) 機構を用いてこの課題を解消。バッファに蓄積した経験をランダムに取り出すこと（経験再生）で経験の自己相関を低減。この手法はオフポリシー手法になら有効。
      - A3Cはオンポリシー手法のため別方法を検討。サンプルを集めるエージェントを並列化することで自己相関を低減することに成功した
- A3Cの課題
  - Python言語の特性上、非同期並列処理を行うのが面倒
    - 後に同期処理を行い性能もA3C に劣らないA2Cがよく利用されるようになる。
  - パフォーマンスを最大化するためには、大規模なリソースを持つ環境が必要
  # 実装の参考
  - https://blog.tensorflow.org/2018/07/deep-reinforcement-learning-keras-eager-execution.htm
  - https://www.amazon.co.jp/つくりながら学ぶ-深層強化学習-PyTorchによる実践プログラミング-株式会社電通国際情報サービス-小川雄太郎/dp/4839965625/ref=sr_1_fkmr1_1?ie=UTF8&qid=1529708595&sr=8-1-fkmr1&keywords=作りながら学ぶ深層
