{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lecture_chap3_exercise_public.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"LnVoFWGNgpzX","colab_type":"text"},"cell_type":"markdown","source":["必要ファイルをダウンロード\n"]},{"metadata":{"id":"2lNAJ9AXgmuf","colab_type":"code","outputId":"f0939d81-1f0f-4686-d9e7-44ff7bc0f044","executionInfo":{"status":"ok","timestamp":1553503612215,"user_tz":-540,"elapsed":15591,"user":{"displayName":"鈴木航介","photoUrl":"","userId":"00468427838777565972"}},"colab":{"base_uri":"https://localhost:8080/","height":1465}},"cell_type":"code","source":["! wget https://www.dropbox.com/s/m60lb68dg6k2im1/config.ini\n","! mkdir model\n","! mkdir model/ntt_output\n","! mkdir data\n","! mkdir data/ntt\n","! wget https://www.dropbox.com/s/t5s09w69mruqr8v/dev.tsv -P data/ntt/\n","! wget https://www.dropbox.com/s/ny2kxiotq0bgqr1/eval.tsv -P data/ntt/\n","! wget https://www.dropbox.com/s/h24zlpk15g1k9sv/train.tsv -P data/ntt/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2019-03-25 08:46:39--  https://www.dropbox.com/s/m60lb68dg6k2im1/config.ini\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/m60lb68dg6k2im1/config.ini [following]\n","--2019-03-25 08:46:39--  https://www.dropbox.com/s/raw/m60lb68dg6k2im1/config.ini\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc3dc838ec61a721892dcfd06f0e.dl.dropboxusercontent.com/cd/0/inline/AdufJIHALUjaTBsFi9B_NKa7TLYgLMv36k9g_EtAQphr_7HpyEfOliFwJuXzxXrTq6arRbftnT_zHqS7QNSRYSXK_6ghCaKNvTZKFNmOtWwBKyvn1qO5Tbkwg722f06vnWg/file# [following]\n","--2019-03-25 08:46:39--  https://uc3dc838ec61a721892dcfd06f0e.dl.dropboxusercontent.com/cd/0/inline/AdufJIHALUjaTBsFi9B_NKa7TLYgLMv36k9g_EtAQphr_7HpyEfOliFwJuXzxXrTq6arRbftnT_zHqS7QNSRYSXK_6ghCaKNvTZKFNmOtWwBKyvn1qO5Tbkwg722f06vnWg/file\n","Resolving uc3dc838ec61a721892dcfd06f0e.dl.dropboxusercontent.com (uc3dc838ec61a721892dcfd06f0e.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n","Connecting to uc3dc838ec61a721892dcfd06f0e.dl.dropboxusercontent.com (uc3dc838ec61a721892dcfd06f0e.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1077 (1.1K) [text/plain]\n","Saving to: ‘config.ini.1’\n","\n","config.ini.1        100%[===================>]   1.05K  --.-KB/s    in 0s      \n","\n","2019-03-25 08:46:40 (133 MB/s) - ‘config.ini.1’ saved [1077/1077]\n","\n","mkdir: cannot create directory ‘model’: File exists\n","mkdir: cannot create directory ‘model/ntt_output’: File exists\n","mkdir: cannot create directory ‘data’: File exists\n","mkdir: cannot create directory ‘data/ntt’: File exists\n","--2019-03-25 08:46:46--  https://www.dropbox.com/s/t5s09w69mruqr8v/dev.tsv\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/t5s09w69mruqr8v/dev.tsv [following]\n","--2019-03-25 08:46:46--  https://www.dropbox.com/s/raw/t5s09w69mruqr8v/dev.tsv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc15c2dee42f5f55bef56eebe69b.dl.dropboxusercontent.com/cd/0/inline/Adv98wdw0kXZ32YcrZnxAe_lsGnyo6eO_WJZbMsUI5ixcYfBzWH13h4APemlUtBBxTy4u21QY5CQ3VSTvgXBsO9w3vNtUYu_ZOCvl0DAaYz8j8xWvHB-AotEKdTLSqtdp-o/file# [following]\n","--2019-03-25 08:46:46--  https://uc15c2dee42f5f55bef56eebe69b.dl.dropboxusercontent.com/cd/0/inline/Adv98wdw0kXZ32YcrZnxAe_lsGnyo6eO_WJZbMsUI5ixcYfBzWH13h4APemlUtBBxTy4u21QY5CQ3VSTvgXBsO9w3vNtUYu_ZOCvl0DAaYz8j8xWvHB-AotEKdTLSqtdp-o/file\n","Resolving uc15c2dee42f5f55bef56eebe69b.dl.dropboxusercontent.com (uc15c2dee42f5f55bef56eebe69b.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n","Connecting to uc15c2dee42f5f55bef56eebe69b.dl.dropboxusercontent.com (uc15c2dee42f5f55bef56eebe69b.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 222195 (217K) [text/plain]\n","Saving to: ‘data/ntt/dev.tsv.1’\n","\n","dev.tsv.1           100%[===================>] 216.99K  --.-KB/s    in 0.08s   \n","\n","2019-03-25 08:46:47 (2.53 MB/s) - ‘data/ntt/dev.tsv.1’ saved [222195/222195]\n","\n","--2019-03-25 08:46:48--  https://www.dropbox.com/s/ny2kxiotq0bgqr1/eval.tsv\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/ny2kxiotq0bgqr1/eval.tsv [following]\n","--2019-03-25 08:46:48--  https://www.dropbox.com/s/raw/ny2kxiotq0bgqr1/eval.tsv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucd0238e2ae3c48e6fbf56b326a0.dl.dropboxusercontent.com/cd/0/inline/Adts2qnKtUC7_w9Dau0Jz4z6Q2UU_JKcK1LhYJpyuv_CtUDQfPSVo9kXv7c2WyEl1c5rgUE5o4AtG4RwGKj720_m0pldmkKVaaBRmzsVOBO98wJiawOf0b4NcE5sDAn4Jto/file# [following]\n","--2019-03-25 08:46:49--  https://ucd0238e2ae3c48e6fbf56b326a0.dl.dropboxusercontent.com/cd/0/inline/Adts2qnKtUC7_w9Dau0Jz4z6Q2UU_JKcK1LhYJpyuv_CtUDQfPSVo9kXv7c2WyEl1c5rgUE5o4AtG4RwGKj720_m0pldmkKVaaBRmzsVOBO98wJiawOf0b4NcE5sDAn4Jto/file\n","Resolving ucd0238e2ae3c48e6fbf56b326a0.dl.dropboxusercontent.com (ucd0238e2ae3c48e6fbf56b326a0.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n","Connecting to ucd0238e2ae3c48e6fbf56b326a0.dl.dropboxusercontent.com (ucd0238e2ae3c48e6fbf56b326a0.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 235514 (230K) [text/plain]\n","Saving to: ‘data/ntt/eval.tsv.1’\n","\n","eval.tsv.1          100%[===================>] 229.99K  --.-KB/s    in 0.09s   \n","\n","2019-03-25 08:46:49 (2.47 MB/s) - ‘data/ntt/eval.tsv.1’ saved [235514/235514]\n","\n","--2019-03-25 08:46:50--  https://www.dropbox.com/s/h24zlpk15g1k9sv/train.tsv\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/h24zlpk15g1k9sv/train.tsv [following]\n","--2019-03-25 08:46:50--  https://www.dropbox.com/s/raw/h24zlpk15g1k9sv/train.tsv\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc37e1338a56499af6e5d487a324.dl.dropboxusercontent.com/cd/0/inline/AdsvGo9re70C1WoO_7IKPfsYAMUeHFFcj6DdOnRdwNsUjN8joCWZrRP4T6oIQAOHT65CyEr2XzCwzoVwyLWV63Wncif0M0TXNCGRy1MY3U5JN4a8o-_D7AgklL_F2m-AVt8/file# [following]\n","--2019-03-25 08:46:50--  https://uc37e1338a56499af6e5d487a324.dl.dropboxusercontent.com/cd/0/inline/AdsvGo9re70C1WoO_7IKPfsYAMUeHFFcj6DdOnRdwNsUjN8joCWZrRP4T6oIQAOHT65CyEr2XzCwzoVwyLWV63Wncif0M0TXNCGRy1MY3U5JN4a8o-_D7AgklL_F2m-AVt8/file\n","Resolving uc37e1338a56499af6e5d487a324.dl.dropboxusercontent.com (uc37e1338a56499af6e5d487a324.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uc37e1338a56499af6e5d487a324.dl.dropboxusercontent.com (uc37e1338a56499af6e5d487a324.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1998277 (1.9M) [text/plain]\n","Saving to: ‘data/ntt/train.tsv.1’\n","\n","train.tsv.1         100%[===================>]   1.91M  11.3MB/s    in 0.2s    \n","\n","2019-03-25 08:46:51 (11.3 MB/s) - ‘data/ntt/train.tsv.1’ saved [1998277/1998277]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"tap-nTRBPIGg","colab_type":"code","outputId":"3cf92c7d-27f3-408c-ba6d-356573baf746","executionInfo":{"status":"ok","timestamp":1553503656858,"user_tz":-540,"elapsed":60215,"user":{"displayName":"鈴木航介","photoUrl":"","userId":"00468427838777565972"}},"colab":{"base_uri":"https://localhost:8080/","height":1941}},"cell_type":"code","source":["! wget https://www.dropbox.com/s/8bdr3wvlufuqj8m/model.ckpt-1000000.index -P model/\n","! wget https://www.dropbox.com/s/iuinyuzrvnkrmp2/model.ckpt-1000000.meta -P model/\n","! wget https://www.dropbox.com/s/01evq5zs9hbspyj/wiki-ja.model -P model/\n","! wget https://www.dropbox.com/s/b464zufwp05pwz4/wiki-ja.vocab -P model/\n","! wget https://www.dropbox.com/s/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001 -P model/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2019-03-25 08:46:52--  https://www.dropbox.com/s/8bdr3wvlufuqj8m/model.ckpt-1000000.index\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/8bdr3wvlufuqj8m/model.ckpt-1000000.index [following]\n","--2019-03-25 08:46:52--  https://www.dropbox.com/s/raw/8bdr3wvlufuqj8m/model.ckpt-1000000.index\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com/cd/0/inline/AdvIqJdbxFv5AeGtuyB12-IbLG3lBxyYR978ZUHWGoQYlnS6L2-N48JtbDfUcx0ooT6UCJbyXorrvQiRv1xRmW-6IO92IBsnzXcDMUNPcqEqlvJ0HwVronzR7QO4MztEjWc/file# [following]\n","--2019-03-25 08:46:53--  https://ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com/cd/0/inline/AdvIqJdbxFv5AeGtuyB12-IbLG3lBxyYR978ZUHWGoQYlnS6L2-N48JtbDfUcx0ooT6UCJbyXorrvQiRv1xRmW-6IO92IBsnzXcDMUNPcqEqlvJ0HwVronzR7QO4MztEjWc/file\n","Resolving ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com (ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n","Connecting to ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com (ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: /cd/0/inline2/AdudLtHfcZSjAmrwJJ6iPgtdbNNRJLFKR4GvltRYIw4tqLsl7Z5pSGjSEl5xy_W49s1G2S0f-ykzuyqqSlzaE8vLWflJh-Cgt962WSBlwBqr5wjpS_9euDhXkVYIMID32tqsXbTclHKGCmCXeJVi-Lj4ct2Srkel38ze3gpIao93VuCdbdXyl04S8rL0DLLalYGUjLVc2QZK2yL3Sl6v6S4MaQ6HrLxgXXe3CS3C5K05d1z55cQZjUTAv42K11mFa4joIi_YDgwLjGVb0bg6mYGa5tFqQkYbhqPZuwcPahR64-LT9QOj3sSb4437_QvbuwSumq9odc1Bh591d-NCEe6nM2Ro9CB-Y_RCxu0IFoIRQw/file [following]\n","--2019-03-25 08:46:53--  https://ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com/cd/0/inline2/AdudLtHfcZSjAmrwJJ6iPgtdbNNRJLFKR4GvltRYIw4tqLsl7Z5pSGjSEl5xy_W49s1G2S0f-ykzuyqqSlzaE8vLWflJh-Cgt962WSBlwBqr5wjpS_9euDhXkVYIMID32tqsXbTclHKGCmCXeJVi-Lj4ct2Srkel38ze3gpIao93VuCdbdXyl04S8rL0DLLalYGUjLVc2QZK2yL3Sl6v6S4MaQ6HrLxgXXe3CS3C5K05d1z55cQZjUTAv42K11mFa4joIi_YDgwLjGVb0bg6mYGa5tFqQkYbhqPZuwcPahR64-LT9QOj3sSb4437_QvbuwSumq9odc1Bh591d-NCEe6nM2Ro9CB-Y_RCxu0IFoIRQw/file\n","Reusing existing connection to ucf47939c618825cb700e5206bac.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 23350 (23K) [application/octet-stream]\n","Saving to: ‘model/model.ckpt-1000000.index.1’\n","\n","model.ckpt-1000000. 100%[===================>]  22.80K  --.-KB/s    in 0.02s   \n","\n","2019-03-25 08:46:53 (949 KB/s) - ‘model/model.ckpt-1000000.index.1’ saved [23350/23350]\n","\n","--2019-03-25 08:46:54--  https://www.dropbox.com/s/iuinyuzrvnkrmp2/model.ckpt-1000000.meta\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/iuinyuzrvnkrmp2/model.ckpt-1000000.meta [following]\n","--2019-03-25 08:46:54--  https://www.dropbox.com/s/raw/iuinyuzrvnkrmp2/model.ckpt-1000000.meta\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc21eb6af3f12cc782a3c8d01694.dl.dropboxusercontent.com/cd/0/inline/AduIrsfkwYWtEa-0oawzYC-b8Jkiq_kLbndgX4J4co1htqS9HLIw6pmteNeoMXCH-acJ4x_PFLPs7msPE-GCPvT5guBYa0XxsmuNwV7aS3n7WwJ19NlI_7brb2Gc_aQfptQ/file# [following]\n","--2019-03-25 08:46:55--  https://uc21eb6af3f12cc782a3c8d01694.dl.dropboxusercontent.com/cd/0/inline/AduIrsfkwYWtEa-0oawzYC-b8Jkiq_kLbndgX4J4co1htqS9HLIw6pmteNeoMXCH-acJ4x_PFLPs7msPE-GCPvT5guBYa0XxsmuNwV7aS3n7WwJ19NlI_7brb2Gc_aQfptQ/file\n","Resolving uc21eb6af3f12cc782a3c8d01694.dl.dropboxusercontent.com (uc21eb6af3f12cc782a3c8d01694.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6018:6::a27d:306\n","Connecting to uc21eb6af3f12cc782a3c8d01694.dl.dropboxusercontent.com (uc21eb6af3f12cc782a3c8d01694.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3938703 (3.8M) [text/plain]\n","Saving to: ‘model/model.ckpt-1000000.meta.1’\n","\n","model.ckpt-1000000. 100%[===================>]   3.76M  20.6MB/s    in 0.2s    \n","\n","2019-03-25 08:46:55 (20.6 MB/s) - ‘model/model.ckpt-1000000.meta.1’ saved [3938703/3938703]\n","\n","--2019-03-25 08:46:56--  https://www.dropbox.com/s/01evq5zs9hbspyj/wiki-ja.model\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/01evq5zs9hbspyj/wiki-ja.model [following]\n","--2019-03-25 08:46:56--  https://www.dropbox.com/s/raw/01evq5zs9hbspyj/wiki-ja.model\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com/cd/0/inline/Adty7UBYZivyx5YudWiCNwGkDgiyNyplJD1woM_eOIrW8SEet8e_AbyTR4qUc2iZGPWi6hq1Pe9gQZE66YnjU9eKL5u5e2_8qWOJbu5oVW-K5xGUPfNmo3fKOJZLEu3shJ0/file# [following]\n","--2019-03-25 08:46:57--  https://uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com/cd/0/inline/Adty7UBYZivyx5YudWiCNwGkDgiyNyplJD1woM_eOIrW8SEet8e_AbyTR4qUc2iZGPWi6hq1Pe9gQZE66YnjU9eKL5u5e2_8qWOJbu5oVW-K5xGUPfNmo3fKOJZLEu3shJ0/file\n","Resolving uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com (uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com (uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: /cd/0/inline2/Adt_O2C5-EQVk8Dgof87LObW9CZ8WUXwz39hnLRN6WenQZ9k1xixIWl6RDrN2OgO_b5zoGHa6a4HLfRmbre23foCidafhFYHOllC91KsNeSSmobh81rvwLOe54c2e8RRgxQUxvLeXJ4_5hrM74vUJcxm7er2T4ITYEt9eSbhympeSLPfPaa2P84WHWiSDmbftJzRbguRaGD-IxkuoeTMfIcpp448WNAXDgJxKmIe2U6OcjMUPD06yNzISisTdDn9ZkZvSSocHfwXiW0pyPAVA8cHerCmLVNbJ7182un_HJ1J5PNWPrzCaKjEtDapymwyNjUjvzVunl0pzmOtyTTZEwnMfkexCELgUeWtIRo0ZclBDQ/file [following]\n","--2019-03-25 08:46:57--  https://uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com/cd/0/inline2/Adt_O2C5-EQVk8Dgof87LObW9CZ8WUXwz39hnLRN6WenQZ9k1xixIWl6RDrN2OgO_b5zoGHa6a4HLfRmbre23foCidafhFYHOllC91KsNeSSmobh81rvwLOe54c2e8RRgxQUxvLeXJ4_5hrM74vUJcxm7er2T4ITYEt9eSbhympeSLPfPaa2P84WHWiSDmbftJzRbguRaGD-IxkuoeTMfIcpp448WNAXDgJxKmIe2U6OcjMUPD06yNzISisTdDn9ZkZvSSocHfwXiW0pyPAVA8cHerCmLVNbJ7182un_HJ1J5PNWPrzCaKjEtDapymwyNjUjvzVunl0pzmOtyTTZEwnMfkexCELgUeWtIRo0ZclBDQ/file\n","Reusing existing connection to uc31d11464826c077929d9a91db1.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 805634 (787K) [application/octet-stream]\n","Saving to: ‘model/wiki-ja.model.1’\n","\n","wiki-ja.model.1     100%[===================>] 786.75K  --.-KB/s    in 0.1s    \n","\n","2019-03-25 08:46:58 (6.24 MB/s) - ‘model/wiki-ja.model.1’ saved [805634/805634]\n","\n","--2019-03-25 08:46:59--  https://www.dropbox.com/s/b464zufwp05pwz4/wiki-ja.vocab\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/b464zufwp05pwz4/wiki-ja.vocab [following]\n","--2019-03-25 08:46:59--  https://www.dropbox.com/s/raw/b464zufwp05pwz4/wiki-ja.vocab\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc6c728cdcdddbff4ae1b846a3cc.dl.dropboxusercontent.com/cd/0/inline/AdvSXM_s4KV6TSqQTcy76ZG-yKaP6wffY00Cv2PlhCZKcK6y1-82YaTkqzUDUZ5PmmTJdILVmWgXilkHd9guF6LxHPdJRI3Mlct_fpgwxzJn3Aml-rltBZohp98_6jDUybc/file# [following]\n","--2019-03-25 08:46:59--  https://uc6c728cdcdddbff4ae1b846a3cc.dl.dropboxusercontent.com/cd/0/inline/AdvSXM_s4KV6TSqQTcy76ZG-yKaP6wffY00Cv2PlhCZKcK6y1-82YaTkqzUDUZ5PmmTJdILVmWgXilkHd9guF6LxHPdJRI3Mlct_fpgwxzJn3Aml-rltBZohp98_6jDUybc/file\n","Resolving uc6c728cdcdddbff4ae1b846a3cc.dl.dropboxusercontent.com (uc6c728cdcdddbff4ae1b846a3cc.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uc6c728cdcdddbff4ae1b846a3cc.dl.dropboxusercontent.com (uc6c728cdcdddbff4ae1b846a3cc.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 595681 (582K) [text/plain]\n","Saving to: ‘model/wiki-ja.vocab.1’\n","\n","wiki-ja.vocab.1     100%[===================>] 581.72K  --.-KB/s    in 0.1s    \n","\n","2019-03-25 08:46:59 (4.36 MB/s) - ‘model/wiki-ja.vocab.1’ saved [595681/595681]\n","\n","--2019-03-25 08:47:00--  https://www.dropbox.com/s/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:601b:1::a27d:801\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001 [following]\n","--2019-03-25 08:47:00--  https://www.dropbox.com/s/raw/bj6itc7edybfq6x/model.ckpt-1000000.data-00000-of-00001\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com/cd/0/inline/AdtQDcKxrmUL6OkxKXPbpUZcdj5pITOgvLeAIwUIgLPCHY9Xy2KXQKltvp5JnwlJlycJMIqUVLZ4pU683pk821nMNeI_HVqMGenoyFeK7jbxElKUu7P8SFm2e_oLeNN8HN0/file# [following]\n","--2019-03-25 08:47:01--  https://uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com/cd/0/inline/AdtQDcKxrmUL6OkxKXPbpUZcdj5pITOgvLeAIwUIgLPCHY9Xy2KXQKltvp5JnwlJlycJMIqUVLZ4pU683pk821nMNeI_HVqMGenoyFeK7jbxElKUu7P8SFm2e_oLeNN8HN0/file\n","Resolving uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com (uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n","Connecting to uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com (uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: /cd/0/inline2/AdvvguRmpyjfotoqLk01_8vC-kSd2_K2b23EHuklgX8Cmvmw5t1MnLff_iUGVau4LUdLsTokCQ2MsoAyqvaaDEbZhOYt-_FsYldRszbVRahLQE4EiVDywcxPrrdy8Slc8boIQ0__8dwEUyxvuzqwnq9vsZMPAnma-k-ubpHX7BALNy74lHcUP9K0m2zjZcOt-aYk6BHZXAFUXNbrFZAJ-Q_zYcbNQppPst2MrEBI_kdq4-xOb7B4uSOdfN77M_Eha1x6JCOxfhQW7Q7zLBPjxT2XvgFlTnkUEmbXtsrGO_YWSuSRh4614TI-kJqKYk41qJk6g5vcpjYvfFmgin6iASmpf079P_PkGqobmhcz8pqtpw/file [following]\n","--2019-03-25 08:47:02--  https://uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com/cd/0/inline2/AdvvguRmpyjfotoqLk01_8vC-kSd2_K2b23EHuklgX8Cmvmw5t1MnLff_iUGVau4LUdLsTokCQ2MsoAyqvaaDEbZhOYt-_FsYldRszbVRahLQE4EiVDywcxPrrdy8Slc8boIQ0__8dwEUyxvuzqwnq9vsZMPAnma-k-ubpHX7BALNy74lHcUP9K0m2zjZcOt-aYk6BHZXAFUXNbrFZAJ-Q_zYcbNQppPst2MrEBI_kdq4-xOb7B4uSOdfN77M_Eha1x6JCOxfhQW7Q7zLBPjxT2XvgFlTnkUEmbXtsrGO_YWSuSRh4614TI-kJqKYk41qJk6g5vcpjYvfFmgin6iASmpf079P_PkGqobmhcz8pqtpw/file\n","Reusing existing connection to uccff55ab244cae102a134054d7c.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1334916128 (1.2G) [application/octet-stream]\n","Saving to: ‘model/model.ckpt-1000000.data-00000-of-00001.1’\n","\n","model.ckpt-1000000. 100%[===================>]   1.24G  41.8MB/s    in 33s     \n","\n","2019-03-25 08:47:36 (38.6 MB/s) - ‘model/model.ckpt-1000000.data-00000-of-00001.1’ saved [1334916128/1334916128]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"nzd02um0TS_p","colab_type":"code","outputId":"78e0a2ac-6f2d-412a-abd5-ba7003b309fe","executionInfo":{"status":"ok","timestamp":1553503663467,"user_tz":-540,"elapsed":66798,"user":{"displayName":"鈴木航介","photoUrl":"","userId":"00468427838777565972"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["! pip install sentencepiece\n","! pip install configparser"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.81)\n","Requirement already satisfied: configparser in /usr/local/lib/python3.6/dist-packages (3.7.4)\n"],"name":"stdout"}]},{"metadata":{"id":"JRw8a9yQe1w5","colab_type":"text"},"cell_type":"markdown","source":["#### Modeling"]},{"metadata":{"id":"NhwkUvRKe1w7","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","import configparser\n","import collections\n","import copy\n","import csv\n","import glob\n","import json\n","import os\n","import sys\n","import tempfile\n","import math\n","import re\n","import numpy as np\n","import sentencepiece as sp\n","import six\n","import tensorflow as tf\n","import pandas as pd\n","\n","\n","class BertConfig(object):\n","\n","  def __init__(self, vocab_size, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\"gelu\", \n","               hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=16, initializer_range=0.02):\n","    \"\"\"\n","      vocab_size: `inputs_ids`の語彙数\n","      hidden_size: encoder層とpooler層の隠れ層次元数, 論文値 768\n","      num_hidden_layers: Transformer encoder層の数, 論文値 12\n","      num_attention_heads: Transformer encoder内部のAttantion headの数, 論文値 12\n","      intermediate_size: Transformer encoder内部のintermediate層(例 feed-forward)の次元数\n","      hidden_act: encoderとpoolerでの活性化関数, 論文値 gelu\n","      hidden_dropout_prob: embeddings, encoder, poolerでのdropout rate\n","      attention_probs_dropout_prob: attention層でのdropout rate\n","      max_position_embeddings: 入力系列の最大長, 論文値 512\n","      type_vocab_size: \n","      initializer_range: 重みパラメタを初期化する際の分散\n","    \"\"\"\n","    self.vocab_size = vocab_size\n","    self.hidden_size = hidden_size\n","    self.num_hidden_layers = num_hidden_layers\n","    self.num_attention_heads = num_attention_heads\n","    self.hidden_act = hidden_act\n","    self.intermediate_size = intermediate_size\n","    self.hidden_dropout_prob = hidden_dropout_prob\n","    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n","    self.max_position_embeddings = max_position_embeddings\n","    self.type_vocab_size = type_vocab_size\n","    self.initializer_range = initializer_range\n","\n","  @classmethod\n","  def from_dict(cls, json_object):\n","    \"\"\"パラメタ情報を持つdictionaryからBERT configを生成する\"\"\"\n","    config = BertConfig(vocab_size=None)\n","    for (key, value) in six.iteritems(json_object):\n","      config.__dict__[key] = value\n","    return config\n","\n","  @classmethod\n","  def from_json_file(cls, json_file):\n","    \"\"\"パラメタ情報を持つjsonからBERT configを生成する\"\"\"\n","    with tf.gfile.GFile(json_file, \"r\") as reader:\n","      text = reader.read()\n","    return cls.from_dict(json.loads(text))\n","\n","  def to_dict(self):\n","    output = copy.deepcopy(self.__dict__)\n","    return output\n","\n","  def to_json_string(self):\n","    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n","\n","\n","class BertModel(object):\n","  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n","\n","  Example usage:\n","\n","  ```python\n","  # Already been converted into WordPiece token ids\n","  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n","  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n","  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n","\n","  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n","    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n","\n","  model = modeling.BertModel(config=config, is_training=True,\n","    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n","\n","  label_embeddings = tf.get_variable(...)\n","  pooled_output = model.get_pooled_output()\n","  logits = tf.matmul(pooled_output, label_embeddings)\n","  ...\n","  ```\n","  \"\"\"\n","\n","  def __init__(self, config, is_training, input_ids, input_mask=None, token_type_ids=None, use_one_hot_embeddings=False, scope=None):\n","    \"\"\"Constructor for BertModel.\n","      config: `BertConfig` インスタンス\n","      is_training: bool. trainingの際はtrue, evalの際はfalse. dropoutの有無を制御する\n","      input_ids: int32 Tensor of shape [batch_size, seq_length]\n","      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length]\n","      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n","      use_one_hot_embeddings: (optional) bool. true:one-hot word embeddings, false:tf.embedding_lookup()\n","      scope: (optional) variable scope. Defaults to \"bert\".\n","    \"\"\"\n","    config = copy.deepcopy(config)\n","    if not is_training:\n","      config.hidden_dropout_prob = 0.0\n","      config.attention_probs_dropout_prob = 0.0\n","\n","    input_shape = get_shape_list(input_ids, expected_rank=2)\n","    batch_size = input_shape[0]\n","    seq_length = input_shape[1]\n","\n","    if input_mask is None:\n","      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n","\n","    if token_type_ids is None:\n","      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n","\n","    with tf.variable_scope(scope, default_name=\"bert\"):\n","      with tf.variable_scope(\"embeddings\"):\n","        # word idsにembedding lookupを適用\n","        (self.embedding_output, self.embedding_table) = embedding_lookup(\n","            input_ids=input_ids, vocab_size=config.vocab_size, embedding_size=config.hidden_size,\n","            initializer_range=config.initializer_range, word_embedding_name=\"word_embeddings\", use_one_hot_embeddings=use_one_hot_embeddings\n","        )\n","        # positional embedding => token type embedding => layer normalize & dropout\n","        self.embedding_output = embedding_postprocessor(\n","            input_tensor=self.embedding_output, use_token_type=True, token_type_ids=token_type_ids, token_type_vocab_size=config.type_vocab_size,token_type_embedding_name=\"token_type_embeddings\",\n","            use_position_embeddings=True, position_embedding_name=\"position_embeddings\", initializer_range=config.initializer_range,\n","            max_position_embeddings=config.max_position_embeddings, dropout_prob=config.hidden_dropout_prob\n","        )\n","\n","      with tf.variable_scope(\"encoder\"):\n","        # [batch_size, seq_length] => [batch_size, seq_length, seq_length]に変換する. これはAttention行列の生成に使用する\n","        attention_mask = create_attention_mask_from_input_mask(input_ids, input_mask)\n","\n","        # Transformerを動かす\n","        self.all_encoder_layers = transformer_model(\n","            input_tensor=self.embedding_output,\n","            attention_mask=attention_mask,\n","            hidden_size=config.hidden_size,\n","            num_hidden_layers=config.num_hidden_layers,\n","            num_attention_heads=config.num_attention_heads,\n","            intermediate_size=config.intermediate_size,\n","            intermediate_act_fn=get_activation(config.hidden_act),\n","            hidden_dropout_prob=config.hidden_dropout_prob,\n","            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n","            initializer_range=config.initializer_range,\n","            do_return_all_layers=True)\n","\n","      self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]\n","    \n","      # `pooler`はencodeされた系列[batch_size, seq_length, hidden_size] を [batch_size, hidden_size]に変換する.\n","      # segmentレベルの分類タスクを解く際に必要になる\n","      with tf.variable_scope(\"pooler\"):\n","        # 最初のトークン[CLS]に当たる内部状態ベクトルを取得する\n","        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1) # 最初のトークンを取得して次元削減\n","        self.pooled_output = tf.layers.dense(first_token_tensor, config.hidden_size, activation=tf.tanh, kernel_initializer=create_initializer(config.initializer_range))\n","\n","  def get_pooled_output(self):\n","    return self.pooled_output\n","\n","  def get_sequence_output(self):\n","    \"\"\"\n","    最後のencoder層の出力を得る\n","\n","    Returns:\n","      float Tensor of shape [batch_size, seq_length, hidden_size]\n","    \"\"\"\n","    return self.sequence_output\n","\n","  def get_all_encoder_layers(self):\n","    return self.all_encoder_layers\n","\n","  def get_embedding_output(self):\n","    \"\"\"\n","    embedding lookupの出力を得る (transformerへの入力になる)\n","    word embedding + positional embedding + token type embeddingを計算したのちlayer normalizationをかける\n","    これはtransformerの入力になる\n","\n","    Returns:\n","      float Tensor of shape [batch_size, seq_length, hidden_size]\n","    \"\"\"\n","    return self.embedding_output\n","\n","  def get_embedding_table(self):\n","    return self.embedding_table\n","\n","\n","# 活性化関数\n","def gelu(x):\n","  \"\"\"\n","  滑らかなReLu\n","  \"\"\"\n","  cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n","  return x * cdf\n","\n","\n","def get_activation(activation_string):\n","  \"\"\"\n","  活性化関数の名前をtfメソッドに変換する\n","  e.g., \"relu\" => `tf.nn.relu`.\n","\n","  Returns:\n","    If `activation_string` is None, empty, or \"linear\", this will return None.\n","    If `activation_string` is not a string, it will return `activation_string`.\n","  \"\"\"\n","  if not isinstance(activation_string, six.string_types):\n","    return activation_string\n","\n","  if not activation_string:\n","    return None\n","\n","  act = activation_string.lower()\n","  if act == \"linear\":\n","    return None\n","  elif act == \"relu\":\n","    return tf.nn.relu\n","  elif act == \"gelu\":\n","    return gelu\n","  elif act == \"tanh\":\n","    return tf.tanh\n","  else:\n","    raise ValueError(\"Unsupported activation: %s\" % act)\n","\n","\n","def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n","  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n","  assignment_map = {}\n","  initialized_variable_names = {}\n","\n","  name_to_variable = collections.OrderedDict()\n","  for var in tvars:\n","    name = var.name\n","    m = re.match(\"^(.*):\\\\d+$\", name)\n","    if m is not None:\n","      name = m.group(1)\n","    name_to_variable[name] = var\n","\n","  init_vars = tf.train.list_variables(init_checkpoint)\n","\n","  assignment_map = collections.OrderedDict()\n","  for x in init_vars:\n","    (name, var) = (x[0], x[1])\n","    if name not in name_to_variable:\n","      continue\n","    assignment_map[name] = name\n","    initialized_variable_names[name] = 1\n","    initialized_variable_names[name + \":0\"] = 1\n","\n","  return (assignment_map, initialized_variable_names)\n","\n","\n","def dropout(input_tensor, dropout_prob):\n","  if dropout_prob is None or dropout_prob == 0.0:\n","    return input_tensor\n","\n","  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n","  return output\n","\n","\n","def layer_norm(input_tensor, name=None):\n","  \"\"\"テンソルの最終次元に対してlayer normalizationを行う\"\"\"\n","  return tf.contrib.layers.layer_norm(inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n","\n","\n","def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n","  output_tensor = layer_norm(input_tensor, name)\n","  output_tensor = dropout(output_tensor, dropout_prob)\n","  return output_tensor\n","\n","\n","def create_initializer(initializer_range=0.02):\n","  \"\"\"与えられたレンジでの切断正規分布を生成する\"\"\"\n","  return tf.truncated_normal_initializer(stddev=initializer_range)\n","\n","\n","def embedding_lookup(input_ids, vocab_size, embedding_size=128, initializer_range=0.02,\n","                     word_embedding_name=\"word_embeddings\", use_one_hot_embeddings=False):\n","  \"\"\"\n","  該当するword idのembeddingを検索する\n","  \n","    input_ids: int32 Tensor of shape [batch_size, seq_length]. word ids.\n","    vocab_size: int.\n","    embedding_size: int.\n","    initializer_range: float. embedding重みパラメタ初期化の際の分散\n","    word_embedding_name: string. embedding tableの名前\n","    use_one_hot_embeddings: bool. True: use one-hot method for word embeddings. False: use `tf.gather()`.\n","\n","  Returns:\n","    float Tensor of shape [batch_size, seq_length, embedding_size].\n","  \"\"\"\n","  if input_ids.shape.ndims == 2: # 入力には３次元のもの[batch_size, seq_length, num_inputs]を想定している\n","    input_ids = tf.expand_dims(input_ids, axis=[-1]) # ２次元のものが来た場合には[batch_size, seq_length, 1]に変換する\n","\n","  embedding_table = tf.get_variable(name=word_embedding_name, shape=[vocab_size, embedding_size], initializer=create_initializer(initializer_range))\n","\n","  flat_input_ids = tf.reshape(input_ids, [-1])\n","  if use_one_hot_embeddings:\n","    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n","    output = tf.matmul(one_hot_input_ids, embedding_table)\n","  else:\n","    output = tf.gather(embedding_table, flat_input_ids)\n","\n","  input_shape = get_shape_list(input_ids)\n","\n","  output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])\n","  return (output, embedding_table)\n","\n","\n","def embedding_postprocessor(input_tensor, use_token_type=False, token_type_ids=None, token_type_vocab_size=16, token_type_embedding_name=\"token_type_embeddings\",\n","                            use_position_embeddings=True, position_embedding_name=\"position_embeddings\", initializer_range=0.02, max_position_embeddings=512, dropout_prob=0.1):\n","  \"\"\"\n","  word embedding tensorにいろいろと後処理を行うメソッド\n","\n","    input_tensor: float Tensor of shape [batch_size, seq_length, embedding_size].\n","    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n","    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length]. `use_token_type`=Trueが必要\n","    token_type_vocab_size: int.\n","    token_type_embedding_name: string. token type embedding table名前\n","    use_position_embeddings: bool. Whether to add position embeddings for the position of each token in the sequence.\n","    position_embedding_name: string. positional embedding tableの名前\n","    initializer_range: float. 重みパラメタ初期化の際の分散\n","    max_position_embeddings: int. 入力系列の最大長\n","    dropout_prob: float. 最終出力のtensorにかけられるdropout rate\n","\n","  Returns:\n","    `input_tensor`と同じサイズのtensorを返す\n","  \"\"\"\n","  input_shape = get_shape_list(input_tensor, expected_rank=3)\n","  batch_size = input_shape[0]\n","  seq_length = input_shape[1]\n","  width = input_shape[2]\n","\n","  output = input_tensor\n","\n","  if use_token_type: # token_type_idsをembeddingsに足し合わせるか\n","    if token_type_ids is None:\n","      raise ValueError(\"`token_type_ids` must be specified if `use_token_type` is True.\")\n","    token_type_table = tf.get_variable(name=token_type_embedding_name, shape=[token_type_vocab_size, width], initializer=create_initializer(initializer_range))\n","    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n","    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size) # onehotのほうが計算が早い\n","    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n","    token_type_embeddings = tf.reshape(token_type_embeddings, [batch_size, seq_length, width])\n","    output += token_type_embeddings\n","\n","  if use_position_embeddings: # position embeddingsをembbeddingsに足し合わせるか\n","    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n","    with tf.control_dependencies([assert_op]):\n","      full_position_embeddings = tf.get_variable(name=position_embedding_name, shape=[max_position_embeddings, width], initializer=create_initializer(initializer_range))\n","      position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n","      num_dims = len(output.shape.as_list())\n","\n","      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n","      # we broadcast among the first dimensions, which is typically just\n","      # the batch size.\n","      position_broadcast_shape = []\n","      for _ in range(num_dims - 2):\n","        position_broadcast_shape.append(1)\n","      position_broadcast_shape.extend([seq_length, width])\n","      position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n","      output += position_embeddings\n","\n","  output = layer_norm_and_dropout(output, dropout_prob)\n","  return output\n","\n","\n","\n","def create_attention_mask_from_input_mask(from_tensor, to_mask):\n","  \"\"\"\n","  2Dのマスク入力から3Dのマスクtensorを出力する\n","\n","  Args:\n","    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n","    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n","\n","  Returns:\n","    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n","  \"\"\"\n","  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n","  batch_size = from_shape[0]\n","  from_seq_length = from_shape[1]\n","\n","  to_shape = get_shape_list(to_mask, expected_rank=2)\n","  to_seq_length = to_shape[1]\n","\n","  to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n","  broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n","  mask = broadcast_ones * to_mask\n","\n","  return mask\n","\n","\n","def attention_layer(from_tensor, to_tensor, attention_mask=None, num_attention_heads=1, size_per_head=512, query_act=None, key_act=None, value_act=None,\n","                    attention_probs_dropout_prob=0.0, initializer_range=0.02, do_return_2d_tensor=False, batch_size=None, from_seq_length=None, to_seq_length=None):\n","  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n","  `from_tensor` と `to_tensor`が同じものの場合self-attentionになる。\n","  `from_tensor` => query\n","  `to_tensor` => key, value\n","  それぞれのtensor shapeは[batch_size, seq_length, size_per_head]\n","\n","    from_tensor: float Tensor of shape [batch_size, from_seq_length, from_width].\n","    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n","    attention_mask: (optional) int32 Tensor of shape [batch_size, from_seq_length, to_seq_length]. 値は0 or 1. 0はmaskする要素に当たる\n","    num_attention_heads: int. attention head の個数. default 1\n","    size_per_head: int. attention headのhidden size\n","    query_act: (optional) Activation function for the query transform.\n","    key_act: (optional) Activation function for the key transform.\n","    value_act: (optional) Activation function for the value transform.\n","    attention_probs_dropout_prob: (optional) float. Dropout probability of the attention probabilities.\n","    initializer_range: float. Range of the weight initializer.\n","    do_return_2d_tensor: bool. True: output shape = [batch_size * from_seq_length, num_attention_heads * size_per_head]\n","                              False: output shape = [batch_size, from_seq_length, num_attention_heads * size_per_head]\n","    batch_size: (Optional) int. 入力が2Dの場合は3Dの`from_tensor`と`to_tensor`のbatch_sizeになる\n","    from_seq_length: (Optional) 入力が2Dの場合は3Dの`from_tensor`のseq_lengthになる\n","    to_seq_length: (Optional) 入力が2Dの場合は3Dの`to_tensor`のseq_lengthになる\n","\n","  Returns:\n","    float Tensor of shape [batch_size, from_seq_length, num_attention_heads * size_per_head]\n","    (`do_return_2d_tensor`=Trueの場合は[batch_size * from_seq_length, num_attention_heads * size_per_head]).\n","  \"\"\"\n","\n","  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,seq_length, width):\n","    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n","    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n","    return output_tensor\n","\n","  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n","  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n","\n","  if len(from_shape) != len(to_shape):\n","    raise ValueError(\"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n","\n","  if len(from_shape) == 3:\n","    batch_size = from_shape[0]\n","    from_seq_length = from_shape[1]\n","    to_seq_length = to_shape[1]\n","  elif len(from_shape) == 2:\n","    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n","      raise ValueError(\n","          \"When passing in rank 2 tensors to attention_layer, the values \"\n","          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n","          \"must all be specified.\")\n","\n","  #   B = batch size (number of sequences)\n","  #   F = `from_tensor` sequence length\n","  #   T = `to_tensor` sequence length\n","  #   N = `num_attention_heads`\n","  #   H = `size_per_head`\n","  from_tensor_2d = reshape_to_matrix(from_tensor)\n","  to_tensor_2d = reshape_to_matrix(to_tensor)\n","\n","  query_layer = tf.layers.dense(\n","      from_tensor_2d,\n","      num_attention_heads * size_per_head,\n","      activation=query_act,\n","      name=\"query\",\n","      kernel_initializer=create_initializer(initializer_range)) # [B*F, N*H]\n","\n","  key_layer = tf.layers.dense(\n","      to_tensor_2d,\n","      num_attention_heads * size_per_head,\n","      activation=key_act,\n","      name=\"key\",\n","      kernel_initializer=create_initializer(initializer_range)) # [B*T, N*H]\n","\n","  value_layer = tf.layers.dense(\n","      to_tensor_2d,\n","      num_attention_heads * size_per_head,\n","      activation=value_act,\n","      name=\"value\",\n","      kernel_initializer=create_initializer(initializer_range)) # [B*T, N*H]\n","\n","  query_layer = transpose_for_scores(query_layer, batch_size,num_attention_heads, from_seq_length, size_per_head) # [B, N, F, H]\n","\n","  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads, to_seq_length, size_per_head) # [B, N, T, H]\n","\n","  # queryとkeyの内積をとってattention scoreを取得する\n","  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) # [B, N, F, T]\n","  attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head))) # scaling\n","\n","  if attention_mask is not None:\n","    attention_mask = tf.expand_dims(attention_mask, axis=[1]) # [B, 1, F, T]\n","\n","    # attention_maskが0の箇所は負の巨大な数値に飛ばすことによってsoftmaxを事実上0にする\n","    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n","    attention_scores += adder\n","\n","  # attention scoreを確率に正規化する\n","  attention_probs = tf.nn.softmax(attention_scores) # [B, N, F, T]\n","  # attentionに対してdropoutを適用する\n","  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n","\n","  value_layer = tf.reshape(value_layer, [batch_size, to_seq_length, num_attention_heads, size_per_head]) # [B, T, N, H]\n","  value_layer = tf.transpose(value_layer, [0, 2, 1, 3]) # [B, N, T, H]\n","\n","  context_layer = tf.matmul(attention_probs, value_layer) # [B, N, F, H]\n","  context_layer = tf.transpose(context_layer, [0, 2, 1, 3]) # [B, F, N, H]\n","\n","  if do_return_2d_tensor:\n","    context_layer = tf.reshape(context_layer, [batch_size * from_seq_length, num_attention_heads * size_per_head]) # [B*F, N*H]\n","  else:\n","    context_layer = tf.reshape(context_layer, [batch_size, from_seq_length, num_attention_heads * size_per_head]) # [B, F, N*H]\n","  return context_layer\n","\n","\n","def transformer_model(input_tensor, attention_mask=None, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072,\n","                      intermediate_act_fn=gelu, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, initializer_range=0.02, do_return_all_layers=False):\n","  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n","  オリジナルのtransformerの実装とほぼ同じ\n","\n","  Also see:\n","  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n","\n","    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n","    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length, seq_length]. 1はAttention対象、0は対象外\n","    hidden_size: int. 隠れ層次元数\n","    num_hidden_layers: int. Transformerの隠れ層の数\n","    num_attention_heads: int. Transformer内部のAttention headの数\n","    intermediate_size: int. feed forward layerの隠れ層次元数\n","    intermediate_act_fn: function. feed forward layerの活性化関数\n","    hidden_dropout_prob: float. hidden layersのdropout rate\n","    attention_probs_dropout_prob: float. attentionのdropout rate\n","    initializer_range: float. 初期化に用いる切断正規分布のレンジ\n","    do_return_all_layer: true->return all layers, false->return final layer\n","\n","  Returns:\n","    float Tensor of shape [batch_size, seq_length, hidden_size], the final hidden layer of the Transformer.\n","  \"\"\"\n","  if hidden_size % num_attention_heads != 0:\n","    raise ValueError(\n","        \"The hidden size (%d) is not a multiple of the number of attention \"\n","        \"heads (%d)\" % (hidden_size, num_attention_heads))\n","\n","  attention_head_size = int(hidden_size / num_attention_heads)\n","  input_shape = get_shape_list(input_tensor, expected_rank=3)\n","  batch_size = input_shape[0]\n","  seq_length = input_shape[1]\n","  input_width = input_shape[2]\n","\n","  # Transformerはresidual addを全layerに対して行うので、入力は隠れ層次元と同じ必要がある.\n","  if input_width != hidden_size:\n","    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" % (input_width, hidden_size))\n","\n","  # 2D形式のtensorを保持する. TPUではreshapeが自在に行えないことが背景にある.\n","  prev_output = reshape_to_matrix(input_tensor)\n","\n","  all_layer_outputs = []\n","  for layer_idx in range(num_hidden_layers):\n","    with tf.variable_scope(\"layer_%d\" % layer_idx):\n","      layer_input = prev_output\n","\n","      # attention layerの定義\n","      with tf.variable_scope(\"attention\"):\n","        attention_heads = []\n","        with tf.variable_scope(\"self\"):\n","          attention_head = attention_layer(from_tensor=layer_input, to_tensor=layer_input, attention_mask=attention_mask, num_attention_heads=num_attention_heads,\n","              size_per_head=attention_head_size, attention_probs_dropout_prob=attention_probs_dropout_prob, initializer_range=initializer_range, do_return_2d_tensor=True,\n","              batch_size=batch_size, from_seq_length=seq_length, to_seq_length=seq_length)\n","          attention_heads.append(attention_head)\n","\n","        # attention headsの出力を結合\n","        attention_output = None\n","        if len(attention_heads) == 1:\n","          attention_output = attention_heads[0]\n","        else:\n","          attention_output = tf.concat(attention_heads, axis=-1)\n","\n","        # FFN->dropout->residual add->layer normalize\n","        with tf.variable_scope(\"output\"):\n","          attention_output = tf.layers.dense(attention_output, hidden_size, kernel_initializer=create_initializer(initializer_range))\n","          attention_output = dropout(attention_output, hidden_dropout_prob)\n","          attention_output = layer_norm(attention_output + layer_input)\n","\n","      # 活性化関数は中間FFNのみにかける\n","      with tf.variable_scope(\"intermediate\"):\n","        intermediate_output = tf.layers.dense(\n","            attention_output,\n","            intermediate_size,\n","            activation=intermediate_act_fn,\n","            kernel_initializer=create_initializer(initializer_range))\n","\n","      # 中間FFNの出力をattention layerに流し、dropout->residual->layer norm\n","      with tf.variable_scope(\"output\"):\n","        layer_output = tf.layers.dense(\n","            intermediate_output,\n","            hidden_size,\n","            kernel_initializer=create_initializer(initializer_range))\n","        layer_output = dropout(layer_output, hidden_dropout_prob)\n","        layer_output = layer_norm(layer_output + attention_output)\n","        prev_output = layer_output\n","        all_layer_outputs.append(layer_output)\n","\n","  if do_return_all_layers:\n","    final_outputs = []\n","    for layer_output in all_layer_outputs:\n","      final_output = reshape_from_matrix(layer_output, input_shape)\n","      final_outputs.append(final_output)\n","    return final_outputs\n","  else:\n","    final_output = reshape_from_matrix(prev_output, input_shape)\n","    return final_output\n","\n","\n","def get_shape_list(tensor, expected_rank=None, name=None):\n","  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n","\n","    tensor: A tf.Tensor object. shapeを取得する対象\n","    expected_rank: (optional) int. shapeを取得した結果こちらのexpected_shapeと異なっていた場合は例外処理を行う\n","    name: (optional) error messageのためのtensorの名前\n","\n","  Returns:\n","    A list of dimensions of the shape of tensor. All static dimensions will\n","    be returned as python integers, and dynamic dimensions will be returned\n","    as tf.Tensor scalars.\n","  \"\"\"\n","  if name is None:\n","    name = tensor.name\n","\n","  if expected_rank is not None:\n","    assert_rank(tensor, expected_rank, name)\n","\n","  shape = tensor.shape.as_list()\n","\n","  non_static_indexes = []\n","  for (index, dim) in enumerate(shape):\n","    if dim is None:\n","      non_static_indexes.append(index)\n","\n","  if not non_static_indexes:\n","    return shape\n","\n","  dyn_shape = tf.shape(tensor)\n","  for index in non_static_indexes:\n","    shape[index] = dyn_shape[index]\n","  return shape\n","\n","\n","def reshape_to_matrix(input_tensor):\n","  \"\"\"rankが2より大きいはあいはrank2のtensorに変形する\"\"\"\n","  ndims = input_tensor.shape.ndims\n","  if ndims < 2:\n","    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n","                     (input_tensor.shape))\n","  if ndims == 2:\n","    return input_tensor\n","\n","  width = input_tensor.shape[-1]\n","  output_tensor = tf.reshape(input_tensor, [-1, width])\n","  return output_tensor\n","\n","\n","def reshape_from_matrix(output_tensor, orig_shape_list):\n","  \"\"\"rank2のtensorをoriginal_shapeに戻す\"\"\"\n","  if len(orig_shape_list) == 2:\n","    return output_tensor\n","\n","  output_shape = get_shape_list(output_tensor)\n","\n","  orig_dims = orig_shape_list[0:-1]\n","  width = output_shape[-1]\n","\n","  return tf.reshape(output_tensor, orig_dims + [width])\n","\n","\n","def assert_rank(tensor, expected_rank, name=None):\n","  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n","\n","    tensor: rankをチェックするtensor入力\n","    expected_rank: Python integer or list of integers, expected rank.\n","    name: (optional) error messageのためのtensorの名前\n","\n","  Raises:\n","    ValueError: expected_shapeが実際のtensorのshapeと合わない場合はエラー\n","  \"\"\"\n","  if name is None:\n","    name = tensor.name\n","\n","  expected_rank_dict = {}\n","  if isinstance(expected_rank, six.integer_types):\n","    expected_rank_dict[expected_rank] = True\n","  else:\n","    for x in expected_rank:\n","      expected_rank_dict[x] = True\n","\n","  actual_rank = tensor.shape.ndims\n","  if actual_rank not in expected_rank_dict:\n","    scope_name = tf.get_variable_scope().name\n","    raise ValueError(\n","        \"For the tensor `%s` in scope `%s`, the actual rank \"\n","        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n","        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nKkprXeze1w_","colab_type":"text"},"cell_type":"markdown","source":["#### Optimizer"]},{"metadata":{"id":"KUq2egKce1xA","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n","  \"\"\"Creates an optimizer training op.\"\"\"\n","  global_step = tf.train.get_or_create_global_step()\n","\n","  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n","\n","  # Implements linear decay of the learning rate.\n","  learning_rate = tf.train.polynomial_decay(\n","      learning_rate,\n","      global_step,\n","      num_train_steps,\n","      end_learning_rate=0.0,\n","      power=1.0,\n","      cycle=False)\n","\n","  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n","  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n","  if num_warmup_steps:\n","    global_steps_int = tf.cast(global_step, tf.int32)\n","    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n","\n","    global_steps_float = tf.cast(global_steps_int, tf.float32)\n","    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n","\n","    warmup_percent_done = global_steps_float / warmup_steps_float\n","    warmup_learning_rate = init_lr * warmup_percent_done\n","\n","    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n","    learning_rate = (\n","        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n","\n","  # It is recommended that you use this optimizer for fine tuning, since this\n","  # is how the model was trained (note that the Adam m/v variables are NOT\n","  # loaded from init_checkpoint.)\n","  optimizer = AdamWeightDecayOptimizer(\n","      learning_rate=learning_rate,\n","      weight_decay_rate=0.01,\n","      beta_1=0.9,\n","      beta_2=0.999,\n","      epsilon=1e-6,\n","      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n","\n","  if use_tpu:\n","    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n","\n","  tvars = tf.trainable_variables()\n","  grads = tf.gradients(loss, tvars)\n","\n","  # This is how the model was pre-trained.\n","  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n","\n","  train_op = optimizer.apply_gradients(\n","      zip(grads, tvars), global_step=global_step)\n","\n","  # Normally the global step update is done inside of `apply_gradients`.\n","  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n","  # a different optimizer, you should probably take this line out.\n","  new_global_step = global_step + 1\n","  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n","  return train_op\n","\n","\n","class AdamWeightDecayOptimizer(tf.train.Optimizer):\n","  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n","\n","  def __init__(self,\n","               learning_rate,\n","               weight_decay_rate=0.0,\n","               beta_1=0.9,\n","               beta_2=0.999,\n","               epsilon=1e-6,\n","               exclude_from_weight_decay=None,\n","               name=\"AdamWeightDecayOptimizer\"):\n","    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n","    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n","\n","    self.learning_rate = learning_rate\n","    self.weight_decay_rate = weight_decay_rate\n","    self.beta_1 = beta_1\n","    self.beta_2 = beta_2\n","    self.epsilon = epsilon\n","    self.exclude_from_weight_decay = exclude_from_weight_decay\n","\n","  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n","    \"\"\"See base class.\"\"\"\n","    assignments = []\n","    for (grad, param) in grads_and_vars:\n","      if grad is None or param is None:\n","        continue\n","\n","      param_name = self._get_variable_name(param.name)\n","\n","      m = tf.get_variable(\n","          name=param_name + \"/adam_m\",\n","          shape=param.shape.as_list(),\n","          dtype=tf.float32,\n","          trainable=False,\n","          initializer=tf.zeros_initializer())\n","      v = tf.get_variable(\n","          name=param_name + \"/adam_v\",\n","          shape=param.shape.as_list(),\n","          dtype=tf.float32,\n","          trainable=False,\n","          initializer=tf.zeros_initializer())\n","\n","      # Standard Adam update.\n","      next_m = (\n","          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n","      next_v = (\n","          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n","                                                    tf.square(grad)))\n","\n","      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n","\n","      # Just adding the square of the weights to the loss function is *not*\n","      # the correct way of using L2 regularization/weight decay with Adam,\n","      # since that will interact with the m and v parameters in strange ways.\n","      #\n","      # Instead we want ot decay the weights in a manner that doesn't interact\n","      # with the m/v parameters. This is equivalent to adding the square\n","      # of the weights to the loss with plain (non-momentum) SGD.\n","      if self._do_use_weight_decay(param_name):\n","        update += self.weight_decay_rate * param\n","\n","      update_with_lr = self.learning_rate * update\n","\n","      next_param = param - update_with_lr\n","\n","      assignments.extend(\n","          [param.assign(next_param),\n","           m.assign(next_m),\n","           v.assign(next_v)])\n","    return tf.group(*assignments, name=name)\n","\n","  def _do_use_weight_decay(self, param_name):\n","    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n","    if not self.weight_decay_rate:\n","      return False\n","    if self.exclude_from_weight_decay:\n","      for r in self.exclude_from_weight_decay:\n","        if re.search(r, param_name) is not None:\n","          return False\n","    return True\n","\n","  def _get_variable_name(self, param_name):\n","    \"\"\"Get the variable name from the tensor name.\"\"\"\n","    m = re.match(\"^(.*):\\\\d+$\", param_name)\n","    if m is not None:\n","      param_name = m.group(1)\n","    return param_name"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z0_jlypYe1xC","colab_type":"text"},"cell_type":"markdown","source":["#### Utils"]},{"metadata":{"id":"viPxpeHGe1xD","colab_type":"code","colab":{}},"cell_type":"code","source":["def str_to_value(input_str):\n","    \"\"\"\n","    Convert data type of value of dict to appropriate one.\n","    Assume there are only three types: str, int, float.\n","    \"\"\"\n","    if input_str.isalpha():\n","        return input_str\n","    elif input_str.isdigit():\n","        return int(input_str)\n","    else:\n","        return float(input_str)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_OdynM84e1xJ","colab_type":"text"},"cell_type":"markdown","source":["#### Sentence piece tokenization"]},{"metadata":{"id":"vCjLnBdoe1xK","colab_type":"code","colab":{}},"cell_type":"code","source":["def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n","    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n","\n","    # The casing has to be passed in by the user and there is no explicit check\n","    # as to whether it matches the checkpoint. The casing information probably\n","    # should have been stored in the bert_config.json file, but it's not, so\n","    # we have to heuristically detect it to validate.\n","\n","    if not init_checkpoint:\n","        return\n","\n","    m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n","    if m is None:\n","        return\n","\n","    model_name = m.group(1)\n","\n","    lower_models = [\n","        \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n","        \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n","    ]\n","\n","    cased_models = [\n","        \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n","        \"multi_cased_L-12_H-768_A-12\"\n","    ]\n","\n","    is_bad_config = False\n","    if model_name in lower_models and not do_lower_case:\n","        is_bad_config = True\n","        actual_flag = \"False\"\n","        case_name = \"lowercased\"\n","        opposite_flag = \"True\"\n","\n","    if model_name in cased_models and do_lower_case:\n","        is_bad_config = True\n","        actual_flag = \"True\"\n","        case_name = \"cased\"\n","        opposite_flag = \"False\"\n","\n","    if is_bad_config:\n","        raise ValueError(\n","            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n","            \"However, `%s` seems to be a %s model, so you \"\n","            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n","            \"how the model was pre-training. If this error is wrong, please \"\n","            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n","                                              model_name, case_name, opposite_flag))\n","\n","\n","def convert_to_unicode(text):\n","    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        elif isinstance(text, unicode):\n","            return text\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def printable_text(text):\n","    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n","\n","    # These functions want `str` for both Python2 and Python3, but in one case\n","    # it's a Unicode string and in the other it's a byte string.\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, unicode):\n","            return text.encode(\"utf-8\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    index = 0\n","    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n","        while True:\n","            token = convert_to_unicode(reader.readline())\n","            if not token:\n","                break\n","            token, _ = token.split(\"\\t\")\n","            token = token.strip()\n","            vocab[token] = index\n","            index += 1\n","    return vocab\n","\n","\n","def convert_by_vocab(vocab, items, unk_info):\n","    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n","    output = []\n","    for item in items:\n","        if item in vocab:\n","            output.append(vocab[item])\n","        else:\n","            output.append(unk_info)\n","    return output\n","\n","\n","def convert_tokens_to_ids(vocab, tokens):\n","    \"\"\"Id of <unk> is assumed as 0 accroding to sentencepiece\"\"\"\n","    return convert_by_vocab(vocab, tokens, unk_info=0)\n","\n","\n","def convert_ids_to_tokens(inv_vocab, ids):\n","    \"\"\"Token of unknown word is assumed as <unk> according to sentencepiece\"\"\"\n","    return convert_by_vocab(inv_vocab, ids, unk_info=\"<unk>\")\n","\n","\n","class FullTokenizer(object):\n","    \"\"\"Runs end-to-end tokenziation.\"\"\"\n","\n","    def __init__(self, model_file, vocab_file, do_lower_case=True):\n","        self.tokenizer = SentencePieceTokenizer(model_file, do_lower_case=do_lower_case)\n","        self.vocab = load_vocab(vocab_file)\n","        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n","\n","    def tokenize(self, text):\n","        split_tokens = self.tokenizer.tokenize(text)\n","        return split_tokens\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        \"\"\"Id of <unk> is assumed as 0 accroding to sentencepiece\"\"\"\n","        return convert_by_vocab(self.vocab, tokens, unk_info=0)\n","\n","    def convert_ids_to_tokens(self, ids):\n","        \"\"\"Token of unknown word is assumed as <unk> according to sentencepiece\"\"\"\n","        return convert_by_vocab(self.inv_vocab, ids, unk_info=\"<unk>\")\n","\n","\n","class SentencePieceTokenizer(object):\n","    \"\"\"Runs SentencePiece tokenization (from raw text to tokens list)\"\"\"\n","\n","    def __init__(self, model_file=None, do_lower_case=True):\n","        \"\"\"Constructs a SentencePieceTokenizer.\"\"\"\n","        self.tokenizer = sp.SentencePieceProcessor()\n","        if self.tokenizer.Load(model_file):\n","            print(\"Loaded a trained SentencePiece model.\")\n","        else:\n","            print(\"You have to give a path of trained SentencePiece model.\")\n","            sys.exit(1)\n","        self.do_lower_case = do_lower_case\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text.\"\"\"\n","        text = convert_to_unicode(text)\n","        if self.do_lower_case:\n","            text = text.lower()\n","        # Tokenization\n","        output_tokens = self.tokenizer.EncodeAsPieces(text)\n","        return output_tokens\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wq34WW-Ye1xO","colab_type":"text"},"cell_type":"markdown","source":["#### A method for running classifier"]},{"metadata":{"id":"sqPOFUjie1xQ","colab_type":"code","outputId":"ac0ad409-fc78-49b9-c3f0-08b841d1ae5e","executionInfo":{"status":"ok","timestamp":1553503667248,"user_tz":-540,"elapsed":70554,"user":{"displayName":"鈴木航介","photoUrl":"","userId":"00468427838777565972"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["CONFIGPATH = './config.ini'\n","config = configparser.ConfigParser()\n","config.read(CONFIGPATH)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./config.ini']"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"oKL1bDpze1xg","colab_type":"code","colab":{}},"cell_type":"code","source":["PRETRAINED_MODEL_PATH = u'./model/model.ckpt-1000000'\n","FINETUNE_OUTPUT_DIR = u'./model/ntt_output'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QZPhHXPDe1xo","colab_type":"code","colab":{}},"cell_type":"code","source":["CONFIGPATH = './config.ini'\n","config = configparser.ConfigParser()\n","config.read(CONFIGPATH)\n","bert_config_file = tempfile.NamedTemporaryFile(mode='w+t', encoding='utf-8', suffix='.json')\n","bert_config_file.write(json.dumps({k:str_to_value(v) for k,v in config['BERT-CONFIG'].items()}))\n","bert_config_file.seek(0)\n","\n","\n","class InputExample(object):\n","  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","  def __init__(self, guid, text_a, text_b=None, label=None):\n","    \"\"\"Constructs a InputExample.\n","\n","    Args:\n","      guid: Unique id for the example.\n","      text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","      text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","      label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","    self.guid = guid\n","    self.text_a = text_a\n","    self.text_b = text_b\n","    self.label = label\n","\n","\n","class PaddingInputExample(object):\n","  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","\n","  When running eval/predict on the TPU, we need to pad the number of examples\n","  to be a multiple of the batch size, because the TPU requires a fixed batch\n","  size. The alternative is to drop the last batch, which is bad because it means\n","  the entire output data won't be generated.\n","\n","  We use this class instead of `None` because treating `None` as padding\n","  battches could cause silent errors.\n","  \"\"\"\n","\n","\n","class InputFeatures(object):\n","  \"\"\"A single set of features of data.\"\"\"\n","\n","  def __init__(self,\n","               input_ids,\n","               input_mask,\n","               segment_ids,\n","               label_id,\n","               is_real_example=True):\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.label_id = label_id\n","    self.is_real_example = is_real_example\n","\n","\n","class DataProcessor(object):\n","  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","  def get_train_examples(self, data_dir):\n","    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","    raise NotImplementedError()\n","\n","  def get_dev_examples(self, data_dir):\n","    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","    raise NotImplementedError()\n","\n","  def get_test_examples(self, data_dir):\n","    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n","    raise NotImplementedError()\n","\n","  def get_labels(self):\n","    \"\"\"Gets the list of labels for this data set.\"\"\"\n","    raise NotImplementedError()\n","\n","  @classmethod\n","  def _read_tsv(cls, input_file, quotechar=None):\n","    \"\"\"Reads a tab separated value file.\"\"\"\n","    with tf.gfile.Open(input_file, \"r\") as f:\n","      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","      lines = []\n","      for line in reader:\n","        lines.append(line)\n","      return lines\n","\n","    \n","# 本講義のためのメソッド.\n","class NTTProcessor(DataProcessor):\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(\n","            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return list(range(24))\n","\n","    @staticmethod\n","    def _create_examples(lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            if i == 0:\n","                idx_text = line.index('query')\n","                idx_label = line.index('label')\n","            else:\n","                guid = \"%s-%s\" % (set_type, i)\n","                text_a = convert_to_unicode(line[idx_text])\n","                label = convert_to_unicode(line[idx_label])\n","                examples.append(\n","                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","        return examples\n","      \n","\n","def convert_single_example(ex_index, example, label_list, max_seq_length,\n","                           tokenizer):\n","  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","\n","  if isinstance(example, PaddingInputExample):\n","    return InputFeatures(\n","        input_ids=[0] * max_seq_length,\n","        input_mask=[0] * max_seq_length,\n","        segment_ids=[0] * max_seq_length,\n","        label_id=0,\n","        is_real_example=False)\n","\n","  label_map = {}\n","  for (i, label) in enumerate(label_list):\n","    label_map[str(label)] = i\n","  tokens_a = tokenizer.tokenize(example.text_a)\n","  tokens_b = None\n","  if example.text_b:\n","    tokens_b = tokenizer.tokenize(example.text_b)\n","\n","  if tokens_b:\n","    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","    # length is less than the specified length.\n","    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","  else:\n","    # Account for [CLS] and [SEP] with \"- 2\"\n","    if len(tokens_a) > max_seq_length - 2:\n","      tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","  # The convention in BERT is:\n","  # (a) For sequence pairs:\n","  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","  # (b) For single sequences:\n","  #  tokens:   [CLS] the dog is hairy . [SEP]\n","  #  type_ids: 0     0   0   0  0     0 0\n","  #\n","  # Where \"type_ids\" are used to indicate whether this is the first\n","  # sequence or the second sequence. The embedding vectors for `type=0` and\n","  # `type=1` were learned during pre-training and are added to the wordpiece\n","  # embedding vector (and position vector). This is not *strictly* necessary\n","  # since the [SEP] token unambiguously separates the sequences, but it makes\n","  # it easier for the model to learn the concept of sequences.\n","  #\n","  # For classification tasks, the first vector (corresponding to [CLS]) is\n","  # used as the \"sentence vector\". Note that this only makes sense because\n","  # the entire model is fine-tuned.\n","  tokens = []\n","  segment_ids = []\n","  tokens.append(\"[CLS]\")\n","  segment_ids.append(0)\n","  for token in tokens_a:\n","    tokens.append(token)\n","    segment_ids.append(0)\n","  tokens.append(\"[SEP]\")\n","  segment_ids.append(0)\n","\n","  if tokens_b:\n","    for token in tokens_b:\n","      tokens.append(token)\n","      segment_ids.append(1)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(1)\n","\n","  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","  # tokens are attended to.\n","  input_mask = [1] * len(input_ids)\n","\n","  # Zero-pad up to the sequence length.\n","  while len(input_ids) < max_seq_length:\n","    input_ids.append(0)\n","    input_mask.append(0)\n","    segment_ids.append(0)\n","\n","  assert len(input_ids) == max_seq_length\n","  assert len(input_mask) == max_seq_length\n","  assert len(segment_ids) == max_seq_length\n","\n","  label_id = label_map[example.label]\n","  if ex_index < 5:\n","    tf.logging.info(\"*** Example ***\")\n","    tf.logging.info(\"guid: %s\" % (example.guid))\n","    tf.logging.info(\"tokens: %s\" % \" \".join(\n","        [printable_text(x) for x in tokens]))\n","    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n","\n","  feature = InputFeatures(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids,\n","      label_id=label_id,\n","      is_real_example=True)\n","  return feature\n","\n","\n","def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):\n","  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n","\n","  writer = tf.python_io.TFRecordWriter(output_file)\n","\n","  for (ex_index, example) in enumerate(examples):\n","    if ex_index % 10000 == 0:\n","      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","    feature = convert_single_example(ex_index, example, label_list,\n","                                     max_seq_length, tokenizer)\n","\n","    def create_int_feature(values):\n","      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n","      return f\n","\n","    features = collections.OrderedDict()\n","    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n","    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n","    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n","    features[\"label_ids\"] = create_int_feature([feature.label_id])\n","    features[\"is_real_example\"] = create_int_feature(\n","        [int(feature.is_real_example)])\n","\n","    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n","    writer.write(tf_example.SerializeToString())\n","  writer.close()\n","\n","\n","def file_based_input_fn_builder(input_file, seq_length, is_training,\n","                                drop_remainder):\n","  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","  name_to_features = {\n","      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n","      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n","      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n","      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n","      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n","  }\n","\n","  def _decode_record(record, name_to_features):\n","    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n","    example = tf.parse_single_example(record, name_to_features)\n","\n","    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n","    # So cast all int64 to int32.\n","    for name in list(example.keys()):\n","      t = example[name]\n","      if t.dtype == tf.int64:\n","        t = tf.to_int32(t)\n","      example[name] = t\n","\n","    return example\n","\n","  def input_fn(params):\n","    \"\"\"The actual input function.\"\"\"\n","    batch_size = params[\"batch_size\"]\n","\n","    # For training, we want a lot of parallel reading and shuffling.\n","    # For eval, we want no shuffling and parallel reading doesn't matter.\n","    d = tf.data.TFRecordDataset(input_file)\n","    if is_training:\n","      d = d.repeat()\n","      d = d.shuffle(buffer_size=100)\n","\n","    d = d.apply(\n","        tf.contrib.data.map_and_batch(\n","            lambda record: _decode_record(record, name_to_features),\n","            batch_size=batch_size,\n","            drop_remainder=drop_remainder))\n","\n","    return d\n","\n","  return input_fn\n","\n","\n","def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","  # This is a simple heuristic which will always truncate the longer sequence\n","  # one token at a time. This makes more sense than truncating an equal percent\n","  # of tokens from each, since if one sequence is very short then each token\n","  # that's truncated likely contains more information than a longer sequence.\n","  while True:\n","    total_length = len(tokens_a) + len(tokens_b)\n","    if total_length <= max_length:\n","      break\n","    if len(tokens_a) > len(tokens_b):\n","      tokens_a.pop()\n","    else:\n","      tokens_b.pop()\n","\n","\n","def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n","                 labels, num_labels, use_one_hot_embeddings):\n","  \"\"\"Creates a classification model.\"\"\"\n","  model = BertModel(\n","      config=bert_config,\n","      is_training=is_training,\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      token_type_ids=segment_ids,\n","      use_one_hot_embeddings=use_one_hot_embeddings)\n","\n","  # In the demo, we are doing a simple classification task on the entire\n","  # segment.\n","  #\n","  # If you want to use the token-level output, use model.get_sequence_output()\n","  # instead.\n","  output_layer = model.get_pooled_output()\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","    if is_training:\n","      # I.e., 0.1 dropout\n","      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    probabilities = tf.nn.softmax(logits, axis=-1)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","\n","    return (loss, per_example_loss, logits, probabilities)\n","\n","\n","def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu,\n","                     use_one_hot_embeddings):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    tf.logging.info(\"*** Features ***\")\n","    for name in sorted(features.keys()):\n","      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","    is_real_example = None\n","    if \"is_real_example\" in features:\n","      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","    else:\n","      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","\n","    (total_loss, per_example_loss, logits, probabilities) = create_model(\n","        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n","        num_labels, use_one_hot_embeddings)\n","\n","    tvars = tf.trainable_variables()\n","    initialized_variable_names = {}\n","    scaffold_fn = None\n","    if init_checkpoint:\n","      (assignment_map, initialized_variable_names\n","       ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","      if use_tpu:\n","\n","        def tpu_scaffold():\n","          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","          return tf.train.Scaffold()\n","\n","        scaffold_fn = tpu_scaffold\n","      else:\n","        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","\n","    tf.logging.info(\"**** Trainable Variables ****\")\n","    for var in tvars:\n","      init_string = \"\"\n","      if var.name in initialized_variable_names:\n","        init_string = \", *INIT_FROM_CKPT*\"\n","      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n","                      init_string)\n","\n","    output_spec = None\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","      train_op = create_optimizer(\n","          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          train_op=train_op,\n","          scaffold_fn=scaffold_fn)\n","    elif mode == tf.estimator.ModeKeys.EVAL:\n","\n","      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n","        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","        accuracy = tf.metrics.accuracy(\n","            labels=label_ids, predictions=predictions, weights=is_real_example)\n","        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"eval_loss\": loss,\n","        }\n","\n","      eval_metrics = (metric_fn,\n","                      [per_example_loss, label_ids, logits, is_real_example])\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          eval_metrics=eval_metrics,\n","          scaffold_fn=scaffold_fn)\n","    else:\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          predictions={\"probabilities\": probabilities},\n","          scaffold_fn=scaffold_fn)\n","    return output_spec\n","\n","  return model_fn\n","\n","\n","# This function is not used by this file but is still used by the Colab and\n","# people who depend on it.\n","def input_fn_builder(features, seq_length, is_training, drop_remainder):\n","  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","  all_input_ids = []\n","  all_input_mask = []\n","  all_segment_ids = []\n","  all_label_ids = []\n","\n","  for feature in features:\n","    all_input_ids.append(feature.input_ids)\n","    all_input_mask.append(feature.input_mask)\n","    all_segment_ids.append(feature.segment_ids)\n","    all_label_ids.append(feature.label_id)\n","\n","  def input_fn(params):\n","    \"\"\"The actual input function.\"\"\"\n","    batch_size = params[\"batch_size\"]\n","\n","    num_examples = len(features)\n","\n","    # This is for demo purposes and does NOT scale to large data sets. We do\n","    # not use Dataset.from_generator() because that uses tf.py_func which is\n","    # not TPU compatible. The right way to load data is with TFRecordReader.\n","    d = tf.data.Dataset.from_tensor_slices({\n","        \"input_ids\":\n","            tf.constant(\n","                all_input_ids, shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"input_mask\":\n","            tf.constant(\n","                all_input_mask,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"segment_ids\":\n","            tf.constant(\n","                all_segment_ids,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"label_ids\":\n","            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n","    })\n","\n","    if is_training:\n","      d = d.repeat()\n","      d = d.shuffle(buffer_size=100)\n","\n","    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","    return d\n","\n","  return input_fn\n","\n","\n","# This function is not used by this file but is still used by the Colab and\n","# people who depend on it.\n","def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n","  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n","\n","  features = []\n","  for (ex_index, example) in enumerate(examples):\n","    if ex_index % 10000 == 0:\n","      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","    feature = convert_single_example(ex_index, example, label_list,\n","                                     max_seq_length, tokenizer)\n","\n","    features.append(feature)\n","  return features\n","\n","\n","def main():\n","  # Fine-tuningを行うためのハイパーパラメタ、その他情報を登録\n","  task_name = 'ntt'\n","  do_train = True\n","  do_eval = True\n","  do_predict = False\n","  data_dir = './data/ntt'\n","  model_file = './model/wiki-ja.model'\n","  vocab_file = './model/wiki-ja.vocab'\n","  init_checkpoint = PRETRAINED_MODEL_PATH\n","  do_lower_case = True\n","  max_seq_length=32\n","  train_batch_size=16\n","  learning_rate=5e-5\n","  num_train_epochs=3.0\n","  output_dir= FINETUNE_OUTPUT_DIR\n","  train_batch_size = 32\n","  eval_batch_size = 8\n","  predict_batch_size = 8\n","  warmup_proportion = 0.1\n","  save_checkpoints_steps = 1000\n","  iterations_per_loop = 1000\n","  use_tpu = False\n","  tpu_name = None\n","  tpu_zone = None\n","  gcp_project = None\n","  master = None\n","  num_tpu_cores = 8\n","  ### end\n","    \n","  tf.logging.set_verbosity(tf.logging.INFO)\n","\n","  processors = {\n","      \"ntt\": NTTProcessor,\n","  }\n","\n","  validate_case_matches_checkpoint(do_lower_case,\n","                                                init_checkpoint)\n","\n","  if not do_train and not do_eval and not do_predict:\n","    raise ValueError(\n","        \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n","\n","  bert_config = BertConfig.from_json_file(bert_config_file.name)\n","\n","  if max_seq_length > bert_config.max_position_embeddings:\n","    raise ValueError(\n","        \"Cannot use sequence length %d because the BERT model \"\n","        \"was only trained up to sequence length %d\" %\n","        (max_seq_length, bert_config.max_position_embeddings))\n","\n","  tf.gfile.MakeDirs(output_dir)\n","\n","  task_name = task_name.lower()\n","\n","  if task_name not in processors:\n","    raise ValueError(\"Task not found: %s\" % (task_name))\n","\n","  processor = processors[task_name]()\n","\n","  label_list = processor.get_labels()\n","\n","  tokenizer = FullTokenizer(\n","      model_file=model_file, vocab_file=vocab_file,\n","      do_lower_case=do_lower_case)\n","\n","  tpu_cluster_resolver = None\n","  if use_tpu and tpu_name:\n","    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n","        tpu_name, zone=tpu_zone, project=gcp_project)\n","\n","  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n","  run_config = tf.contrib.tpu.RunConfig(\n","      cluster=tpu_cluster_resolver,\n","      master=master,\n","      model_dir=output_dir,\n","      save_checkpoints_steps=save_checkpoints_steps,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=iterations_per_loop,\n","          num_shards=num_tpu_cores,\n","          per_host_input_for_training=is_per_host))\n","\n","  train_examples = None\n","  num_train_steps = None\n","  num_warmup_steps = None\n","  if do_train:\n","    train_examples = processor.get_train_examples(data_dir)\n","    num_train_steps = int(\n","        len(train_examples) / train_batch_size * num_train_epochs)\n","    num_warmup_steps = int(num_train_steps * warmup_proportion)\n","\n","  model_fn = model_fn_builder(\n","      bert_config=bert_config,\n","      num_labels=len(label_list),\n","      init_checkpoint=init_checkpoint,\n","      learning_rate=learning_rate,\n","      num_train_steps=num_train_steps,\n","      num_warmup_steps=num_warmup_steps,\n","      use_tpu=use_tpu,\n","      use_one_hot_embeddings=use_tpu)\n","\n","  # If TPU is not available, this will fall back to normal Estimator on CPU\n","  # or GPU.\n","  estimator = tf.contrib.tpu.TPUEstimator(\n","      use_tpu=use_tpu,\n","      model_fn=model_fn,\n","      config=run_config,\n","      train_batch_size=train_batch_size,\n","      eval_batch_size=eval_batch_size,\n","      predict_batch_size=predict_batch_size)\n","\n","  if do_train:\n","    train_file = os.path.join(output_dir, \"train.tf_record\")\n","    file_based_convert_examples_to_features(\n","        train_examples, label_list, max_seq_length, tokenizer, train_file)\n","    tf.logging.info(\"***** Running training *****\")\n","    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n","    tf.logging.info(\"  Batch size = %d\", train_batch_size)\n","    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n","    train_input_fn = file_based_input_fn_builder(\n","        input_file=train_file,\n","        seq_length=max_seq_length,\n","        is_training=True,\n","        drop_remainder=True)\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","\n","  if do_eval:\n","    eval_examples = processor.get_dev_examples(data_dir)\n","    num_actual_eval_examples = len(eval_examples)\n","    if use_tpu:\n","      # TPU requires a fixed batch size for all batches, therefore the number\n","      # of examples must be a multiple of the batch size, or else examples\n","      # will get dropped. So we pad with fake examples which are ignored\n","      # later on. These do NOT count towards the metric (all tf.metrics\n","      # support a per-instance weight, and these get a weight of 0.0).\n","      while len(eval_examples) % eval_batch_size != 0:\n","        eval_examples.append(PaddingInputExample())\n","\n","    eval_file = os.path.join(output_dir, \"eval.tf_record\")\n","    file_based_convert_examples_to_features(\n","        eval_examples, label_list, max_seq_length, tokenizer, eval_file)\n","\n","    tf.logging.info(\"***** Running evaluation *****\")\n","    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n","                    len(eval_examples), num_actual_eval_examples,\n","                    len(eval_examples) - num_actual_eval_examples)\n","    tf.logging.info(\"  Batch size = %d\", eval_batch_size)\n","\n","    # This tells the estimator to run through the entire set.\n","    eval_steps = None\n","    # However, if running eval on the TPU, you will need to specify the\n","    # number of steps.\n","    if use_tpu:\n","      assert len(eval_examples) % eval_batch_size == 0\n","      eval_steps = int(len(eval_examples) // eval_batch_size)\n","\n","    eval_drop_remainder = True if use_tpu else False\n","    eval_input_fn = file_based_input_fn_builder(\n","        input_file=eval_file,\n","        seq_length=max_seq_length,\n","        is_training=False,\n","        drop_remainder=eval_drop_remainder)\n","\n","    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","\n","    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n","    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n","      tf.logging.info(\"***** Eval results *****\")\n","      for key in sorted(result.keys()):\n","        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n","        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","  if do_predict:\n","    predict_examples = processor.get_test_examples(data_dir)\n","    num_actual_predict_examples = len(predict_examples)\n","    if use_tpu:\n","      # TPU requires a fixed batch size for all batches, therefore the number\n","      # of examples must be a multiple of the batch size, or else examples\n","      # will get dropped. So we pad with fake examples which are ignored\n","      # later on.\n","      while len(predict_examples) % predict_batch_size != 0:\n","        predict_examples.append(PaddingInputExample())\n","\n","    predict_file = os.path.join(output_dir, \"predict.tf_record\")\n","    file_based_convert_examples_to_features(predict_examples, label_list,\n","                                            max_seq_length, tokenizer,\n","                                            predict_file)\n","\n","    tf.logging.info(\"***** Running prediction*****\")\n","    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n","                    len(predict_examples), num_actual_predict_examples,\n","                    len(predict_examples) - num_actual_predict_examples)\n","    tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n","\n","    predict_drop_remainder = True if use_tpu else False\n","    predict_input_fn = file_based_input_fn_builder(\n","        input_file=predict_file,\n","        seq_length=max_seq_length,\n","        is_training=False,\n","        drop_remainder=predict_drop_remainder)\n","\n","    result = estimator.predict(input_fn=predict_input_fn)\n","\n","    output_predict_file = os.path.join(output_dir, \"test_results.tsv\")\n","    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n","      num_written_lines = 0\n","      tf.logging.info(\"***** Predict results *****\")\n","      for (i, prediction) in enumerate(result):\n","        probabilities = prediction[\"probabilities\"]\n","        if i >= num_actual_predict_examples:\n","          break\n","        output_line = \"\\t\".join(\n","            str(class_probability)\n","            for class_probability in probabilities) + \"\\n\"\n","        writer.write(output_line)\n","        num_written_lines += 1\n","    assert num_written_lines == num_actual_predict_examples"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SqGmSzSMe1xq","colab_type":"code","outputId":"d1d0c462-0ef6-44ff-a24c-fb4c17d82550","executionInfo":{"status":"ok","timestamp":1553504651412,"user_tz":-540,"elapsed":1054693,"user":{"displayName":"鈴木航介","photoUrl":"","userId":"00468427838777565972"}},"colab":{"base_uri":"https://localhost:8080/","height":10322}},"cell_type":"code","source":["main()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Loaded a trained SentencePiece model.\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f6ea6f15840>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': './model/ntt_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ea6ac8d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n","INFO:tensorflow:Writing example 0 of 18556\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: train-1\n","INFO:tensorflow:tokens: [CLS] ▁ コンビニ の 駐車場 に 駐車 していた ら 当 逃げ された [SEP]\n","INFO:tensorflow:input_ids: 4 9 23314 10 6298 17 26334 133 154 905 3160 53 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 19 (id = 19)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: train-2\n","INFO:tensorflow:tokens: [CLS] ▁ 縁 石 に 衝突 [SEP]\n","INFO:tensorflow:input_ids: 4 9 1540 284 17 3377 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: train-3\n","INFO:tensorflow:tokens: [CLS] ▁ 片側 一 車線 の 右 カーブ を 進行 中 荷 台 から ベニ ヤ 板 が 落下 し 歩行者 に当たった [SEP]\n","INFO:tensorflow:input_ids: 4 9 13528 92 6188 10 819 8433 18 4376 82 6354 471 28 18190 485 1001 12 9356 32 17027 19136 5 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 21 (id = 21)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: train-4\n","INFO:tensorflow:tokens: [CLS] ▁ トンネル を 出た ところで 、 フロント ガラス に 石 が 飛んで きて ヒ ビ が入る [SEP]\n","INFO:tensorflow:input_ids: 4 9 2353 18 7556 6938 7 4591 3784 17 284 12 26956 17068 523 365 8797 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 10 (id = 10)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: train-5\n","INFO:tensorflow:tokens: [CLS] ▁ 狭い 道 で 対 向 車が きて 横 に よ った 。 ガード レール に 接触 して す った ような 気 が したが その時 は 確認 しなかった 。 次の [SEP]\n","INFO:tensorflow:input_ids: 4 9 8312 162 19 618 1669 11092 17068 894 17 842 201 8 5682 5001 17 6676 55 263 201 877 474 12 679 15841 11 2445 2442 8 1497 5\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:Writing example 10000 of 18556\n","INFO:tensorflow:***** Running training *****\n","INFO:tensorflow:  Num examples = 18556\n","INFO:tensorflow:  Batch size = 32\n","INFO:tensorflow:  Num steps = 1739\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-10-16eb9539b411>:299: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.map_and_batch(...)`.\n","WARNING:tensorflow:From <ipython-input-10-16eb9539b411>:279: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Running train on CPU\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (32, 32)\n","INFO:tensorflow:  name = input_mask, shape = (32, 32)\n","INFO:tensorflow:  name = is_real_example, shape = (32,)\n","INFO:tensorflow:  name = label_ids, shape = (32,)\n","INFO:tensorflow:  name = segment_ids, shape = (32, 32)\n","WARNING:tensorflow:From <ipython-input-4-2327fc9afd36>:262: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From <ipython-input-4-2327fc9afd36>:462: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","INFO:tensorflow:**** Trainable Variables ****\n","INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (32000, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = output_weights:0, shape = (24, 768)\n","INFO:tensorflow:  name = output_bias:0, shape = (24,)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from ./model/ntt_output/model.ckpt-0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into ./model/ntt_output/model.ckpt.\n","INFO:tensorflow:global_step/sec: 1.82548\n","INFO:tensorflow:examples/sec: 58.4155\n","INFO:tensorflow:global_step/sec: 1.95223\n","INFO:tensorflow:examples/sec: 62.4713\n","INFO:tensorflow:global_step/sec: 1.95337\n","INFO:tensorflow:examples/sec: 62.5079\n","INFO:tensorflow:global_step/sec: 1.94486\n","INFO:tensorflow:examples/sec: 62.2354\n","INFO:tensorflow:global_step/sec: 1.9578\n","INFO:tensorflow:examples/sec: 62.6497\n","INFO:tensorflow:global_step/sec: 1.95923\n","INFO:tensorflow:examples/sec: 62.6953\n","INFO:tensorflow:global_step/sec: 1.95016\n","INFO:tensorflow:examples/sec: 62.4052\n","INFO:tensorflow:global_step/sec: 1.96641\n","INFO:tensorflow:examples/sec: 62.9251\n","INFO:tensorflow:global_step/sec: 1.9826\n","INFO:tensorflow:examples/sec: 63.443\n","INFO:tensorflow:Saving checkpoints for 1000 into ./model/ntt_output/model.ckpt.\n","INFO:tensorflow:global_step/sec: 1.64813\n","INFO:tensorflow:examples/sec: 52.7401\n","INFO:tensorflow:global_step/sec: 1.94724\n","INFO:tensorflow:examples/sec: 62.3116\n","INFO:tensorflow:global_step/sec: 1.94561\n","INFO:tensorflow:examples/sec: 62.2595\n","INFO:tensorflow:global_step/sec: 1.94952\n","INFO:tensorflow:examples/sec: 62.3846\n","INFO:tensorflow:global_step/sec: 1.94982\n","INFO:tensorflow:examples/sec: 62.3942\n","INFO:tensorflow:global_step/sec: 1.96213\n","INFO:tensorflow:examples/sec: 62.7881\n","INFO:tensorflow:global_step/sec: 1.95016\n","INFO:tensorflow:examples/sec: 62.4051\n","INFO:tensorflow:global_step/sec: 1.9499\n","INFO:tensorflow:examples/sec: 62.3968\n","INFO:tensorflow:Saving checkpoints for 1739 into ./model/ntt_output/model.ckpt.\n","INFO:tensorflow:Loss for final step: 0.008470161.\n","INFO:tensorflow:training_loop marked as finished\n","INFO:tensorflow:Writing example 0 of 2061\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: dev-1\n","INFO:tensorflow:tokens: [CLS] ▁ 単独で スリップ し 横 倒 し となった 。 対 物 賠償 不要 。 [SEP]\n","INFO:tensorflow:input_ids: 4 9 8257 26689 32 894 3515 32 72 8 618 280 18005 9366 8 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: dev-2\n","INFO:tensorflow:tokens: [CLS] ▁ 国際線 駐車場 に 駐車 し 、 2 時間 後 に戻った ら 、 フロント ガラス に ひ び が 入 っていた 。 [SEP]\n","INFO:tensorflow:input_ids: 4 9 24796 6298 17 26334 32 7 25 364 108 5386 154 7 4591 3784 17 1155 1417 12 682 921 8 5 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 13 (id = 13)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: dev-3\n","INFO:tensorflow:tokens: [CLS] ▁ 教会の 門 に 接触 。 [SEP]\n","INFO:tensorflow:input_ids: 4 9 7204 462 17 6676 8 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: dev-4\n","INFO:tensorflow:tokens: [CLS] ▁ 走行 中に 左 に 寄り 過ぎ 左側 の バン バー を 石 に ぶ つけた 。 [SEP]\n","INFO:tensorflow:input_ids: 4 9 3212 747 801 17 4206 6984 11923 10 1011 394 18 284 17 1028 7430 8 5 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: dev-5\n","INFO:tensorflow:tokens: [CLS] ▁ 駐車場 から 道路 に出る 際 誤って 駐車 中の 相手 車 に 接触 。 [SEP]\n","INFO:tensorflow:input_ids: 4 9 6298 28 666 9309 1962 17486 26334 853 2245 304 17 6676 8 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 12 (id = 12)\n","INFO:tensorflow:***** Running evaluation *****\n","INFO:tensorflow:  Num examples = 2061 (2061 actual, 0 padding)\n","INFO:tensorflow:  Batch size = 8\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Running eval on CPU\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (?, 32)\n","INFO:tensorflow:  name = input_mask, shape = (?, 32)\n","INFO:tensorflow:  name = is_real_example, shape = (?,)\n","INFO:tensorflow:  name = label_ids, shape = (?,)\n","INFO:tensorflow:  name = segment_ids, shape = (?, 32)\n","INFO:tensorflow:**** Trainable Variables ****\n","INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (32000, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","INFO:tensorflow:  name = output_weights:0, shape = (24, 768)\n","INFO:tensorflow:  name = output_bias:0, shape = (24,)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2019-03-25T09:03:56Z\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from ./model/ntt_output/model.ckpt-1739\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Finished evaluation at 2019-03-25-09:04:10\n","INFO:tensorflow:Saving dict for global step 1739: eval_accuracy = 0.9262494, eval_loss = 0.2949049, global_step = 1739, loss = 0.29447883\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1739: ./model/ntt_output/model.ckpt-1739\n","INFO:tensorflow:evaluation_loop marked as finished\n","INFO:tensorflow:***** Eval results *****\n","INFO:tensorflow:  eval_accuracy = 0.9262494\n","INFO:tensorflow:  eval_loss = 0.2949049\n","INFO:tensorflow:  global_step = 1739\n","INFO:tensorflow:  loss = 0.29447883\n"],"name":"stdout"}]},{"metadata":{"id":"MKOPtue5dBm9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}