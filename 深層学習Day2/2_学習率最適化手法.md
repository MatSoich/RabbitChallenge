<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


# 学習率最適化手法
- 勾配降下法などの学習率をどう決めるかの手法
- 学習率を固定させず変化させていく

# モメンタム
- 勾配降下方がジグザグに動くのに対して、株価の移動平均のような動き方をする。
> \\\(V_t = \mu V_{t-1} - \epsilon \nabla E\\\)
> \\\(w^{(t+1)} = w^{(t)} + V_t\\\)
- 誤差をパラメータで微分したものと学習率の席を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する。
- モメンタムのメリット
  - 局所最適解にならず大域的最適解となる。
  - 谷間についてから最も低い位置（最適値）に行くまでの時間が早い。

# AdaGrad
- 勾配の緩やかな問題に有効だが、鞍点問題に陥る危険性がある。
> \\\(h_0 = \theta\\\)
> \\\(h_t = h_{t-1} + (\nabla E)^2\\\)
> \\\(\displaystyle w^{(t+1)} = w^{(t)} - \epsilon \frac{1}{\sqrt{h_t} + \theta} \nabla E\\\)
- 誤差をパラメータで微分したものと再定義した学習率の積を減算している。
- AdaGradのメリット
  - 勾配の緩やかな斜面に対して、最適値に近づける
- 課題
  - 学習率が徐々に小さくなるので、鞍点問題を引き起こすことがあった。

# RMSProp
- AdaGradを改良したもの
> \\\(h_t = ah_{t-1} + (1-a)(\nabla E)^2\\\)
> \\\(\displaystyle w^{(t+1)} = w^{(t)} - \epsilon \frac{1}{\sqrt{h_t} + \theta} \nabla E\\\)
- パラメータ更新の式はadagradと同一。
- 学習率更新の工夫により、RMSpropはハイパーパラメータの調整が必要な場合が少ない。

# Adam
- モメンタムとAdaGrad-RMSProp系列のハイブリッド
-  以下のミックス
   - モメンタムの、過去の勾配の指数関数的減衰平均
   - RMSPropの、過去の勾配の２乗の指数関数的減衰平均
- 最も実用的なモデル
- 鞍点問題も抜けることができる
- 学習率が乱高下なく進むのも良いポイント

# 実装
