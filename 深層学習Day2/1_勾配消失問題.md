<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

勾配消失問題
============
# 勾配消失問題
- 誤差逆伝播法がうまくいかない現象
  - 誤差逆伝播法では（多くの関数の場合、微分を続けると関数の取り得る値の最大値がどんどん小さくなるため、）下位層に沈んでいくにつれて勾配がどんどん緩やかになっていき、下位層のパラメータでは更新でほとんど値が変わらなくなる。そのため、訓練が最適解に就職しなくなる。

# 確認テスト１４（誤差逆伝播法の復習）
> \\\(\begin{aligned}z &= t^2\cr
t &= x + y\cr
\displaystyle \frac{dz}{dt} &= 2t\cr
\displaystyle \frac{dt}{dx} &= 1\end{aligned}\\\)
> 連鎖率の原理より<br>
> \\\(\displaystyle \frac{dz}{dx} = \frac{dz}{dt}\frac{dt}{dx}\\\)<br>
> \\\(\displaystyle \frac{dz}{dx} = 2t \cdot 1 = 2(x + y)\\\)<br>
#　勾配消失問題（続き）
- シグモイド関数などが代表的な例。
  >（シグモイド関数の微分を記入）

# 確認テスト１５
- シグモイド関数を微分したとき、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。
> \\\(\displaystyle f(u)=\frac{1}{1+e^{-u}}\\\)<br>
> \\\(\begin{aligned}\displaystyle f^\prime(u) &=\frac{1}{1+e^{-u}} \cr &= - \frac{1}{(1+e^{-u})^2}(- e^{-u}) \cr &=\frac{(1 + e^{-u}) -1}{(1+e^{-u})^2} \cr &=\frac{1}{1+e^{-u}}\left(1 - \frac{1}{1+e^{-u}} \right) \cr &=f(u)\left\lbrace 1 - f(u) \right\rbrace \end{aligned} \\\)<br>
> \\\(f(0) = \displaystyle \frac{1}{1+e^{0}} = \frac{1}{2}\\\)<br>
> \\\(\begin{aligned} \displaystyle f^\prime(0) =  f(0) \lbrace1 - f(0) \rbrace =　\frac{1}{4} \end{aligned} \\\)<br>

- よって正解の値は（２）0.25

#　勾配消失問題の解決法
- 以下の３つの方法がある。
  - 活性化関数の選択
  - 重みの初期値設定
  - バッチ正規化

# 活性化関数の選択
- 勾配消失問題が起こりにくい活性化関数の選択
  - 例としてReLU関数（一番使われている）
    - x>０ or notでそのまま伝播するか、全く伝播しないか決まる。
      - 必要な部分だけ選択されるような動きになる。
      - スパース化に貢献

# 重みの初期値設定
- 重みにいろいろな個性を持たせて、様々な観点の特徴量を抽出することが重要。
- Xavierの初期化
  - Xavierの初期化以前は標準化されていた。（勾配消失問題が発生）
  - Xavierは前のレイヤーのノードの数のルートで割ることで重みを初期化し、勾配消失問題を避ける。（なぜか）
  - シグモイド関数のようなS字カーブを描いている活性化関数についてはXavierの初期化はよく働く。
- Heの初期化
  - ReLU関数のようなS字カーブを描かない活性化関数に有効
  - 前のレイヤーのノードの数のルートで割った後、ルート２をかけることで初期化する。

# 確認テスト１６
- 重みの初期値を０にするとどのような問題が発生するか。簡潔に説明せよ。
  - 正しい学習が行えない。
  - 全ての重みの値が均一に更新されるため、多様な重みを持つ意味がなくなる。

# バッチ正規化
- ミニバッチ単位で入力値のデータの偏りを抑制する方法
- 使いどころ
  - 活性化関数に値を渡す前後に、バッチ正規化の処理を含んだ層を加える。
- バッチ正規化を使用することで中間層が安定化して学習がスピードアップする。
- バッチ正規化の数学的記述は以下。
> \\\(\displaystyle \mu_t = \frac{1}{N_t} \sum_{i =1}^{N_t}x_{ni}\\\)<br>
> \\\(\displaystyle \sigma_t^2 = \frac{1}{N_t} \sum_{i =1}^{N_t}(x_{ni} - \mu_t)^2\\\)<br>
> \\\(\displaystyle \hat x_{ni} = \frac{x_{ni}-\mu_t}{\sqrt{\sigma_t^2 + \theta}}\\\)<br>
> \\\(y_{mi} = \gamma x_{ni} + \beta\\\)<br>

# 実装
- コードは2_2_2の中に、以下３つのニューラルネットワークがある。
    1. Gauss正規化で活性化関数はSigmoid
    2. Gauss正規化で活性化関数はReLU
    3. Xavier正規化で活性化関数はSigmoid
    - １と２の比較から活性化関数の選択
    - １と３の違いから重みの初期値設定の重要性がわかる。
  - コード2_3の中に、バッチ正規化を実施したニューラルネットワークがある。
    - 結果を見ると、学習が進んでいることがわかる。




