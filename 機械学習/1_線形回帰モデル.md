機械学習
============
# 機械学習モデリングプロセス

- 問題設定
  - 最も重要
  - 機械学習が必要なのか
- データ選定
  - GIGO(Garbage in Garbage out)になってないか
- データの前処理
  - 時間の90％
- 機械学習モデルの選定（ここのメイン）

# 扱う内容について
- 教師なし学習と教師あり学習について

# 機械学習

- 機械学習の定義
  - トム・ミッチェルなどによる定義

# 線形回帰モデル
- 線形回帰とは
  - ざっくりいえば比例、超平面、など
  - y=ax+b（2次元）、y=ax+by+c(3次元)、、、、
  - 一般にはy=a0 + Σax
- 回帰問題
  - ある入力（離散あるいは連続）から出力（連続値）を予測
  - バプニックの原理
    - 回帰ではランキング的なことも考えることができるが、回帰はランキングだけでなく、差なんかも考えている。
    - なので、ランキングを回帰問題で考えることは、途中でより難しい問題を考えていることであり、望ましくない。
    - 密度比推定で調べると面白いし差が色々得られる。東京大学の杉山先生など
  - 入力とパラメータの内積として、モデルを表せる。
  - 誤差について
    - ホワイトノイズだけでなく、説明変数で拾い切れていない要素もここに入る。
  - 入出力全体をベクトルや線形代数を使ってまとめて表現することがある。
  - y＝Xw として、Xは計画行列（デザインマトリックス）といわれる。
- データの分割とモデルの汎化性能測定について
  - 汎化(Generalization)のために推論データと検証データに分割させる
  - クロスバリデーションなど
- 線形回帰のモデルのパラメータの推定
  - 最小二乗法で推定
  - Σ(y_ihat - y_i)^2 = Σ(ε_i)^2
  - なぜならy_ihat = w_0 + w_1 * x_i, y_i = w_0 + w_1 * x_i + ε_i
  - 注意点として、二乗誤差は外れ値に弱い
  - Huber損失、Tukey損失というのように外れ値に頑健な機械学習を考えることはできる。（イラストで学ぶ機械学習）
  - MSE_i = J(w) = Σ(ε_i)^2/nが今我々が予測したいこと
  - MSEが最小となるwの値を微分して求める。
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\frac{\partial&space;}{\partial&space;w}&space;\left&space;\{&space;\frac{1}{n_{train}}\sum_{i=1}^{n_{train}}&space;\left&space;(&space;\boldsymbol{x_{i}^{T}&space;w}&space;-&space;\boldsymbol{y_{i}}&space;\right&space;)^2&space;\right&space;\}&space;=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\frac{\partial&space;}{\partial&space;w}&space;\left&space;\{&space;\frac{1}{n_{train}}\sum_{i=1}^{n_{train}}&space;\left&space;(&space;\boldsymbol{x_{i}^{T}&space;w}&space;-&space;y_{i}&space;\right&space;)^2&space;\right&space;\}&space;=0" title="\small \frac{\partial }{\partial w} \left \{ \frac{1}{n_{train}}\sum_{i=1}^{n_{train}} \left ( \boldsymbol{x_{i}^{T} w} - \boldsymbol{y_{i}} \right )^2 \right \} =0" /></a>
  - 変形して
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\frac{\partial&space;}{\partial&space;w}&space;\left&space;\{&space;\frac{1}{n_{train}}&space;\left&space;(&space;\boldsymbol{Xw}&space;-&space;\boldsymbol{Y}&space;\right&space;)^T&space;\left&space;(&space;\boldsymbol{Xw}&space;-&space;\boldsymbol{Y}&space;\right&space;)\right&space;\}&space;=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\frac{\partial&space;}{\partial&space;w}&space;\left&space;\{&space;\frac{1}{n_{train}}&space;\left&space;(&space;\boldsymbol{Xw}&space;-&space;\boldsymbol{y}&space;\right&space;)^T&space;\left&space;(&space;\boldsymbol{Xw}&space;-&space;\boldsymbol{y}&space;\right&space;)\right&space;\}&space;=0" title="\small \frac{\partial }{\partial w} \left \{ \frac{1}{n_{train}} \left ( \boldsymbol{Xw} - \boldsymbol{y} \right )^T \left ( \boldsymbol{Xw} - \boldsymbol{Y} \right )\right \} =0" /></a>
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\frac{\partial&space;}{\partial&space;w}&space;\left&space;\{&space;\frac{1}{n_{train}}&space;\left&space;(&space;\boldsymbol{w^{T}X^{T}Xw}&space;-&space;\boldsymbol{y^{T}Xw}&space;-&space;\boldsymbol{w^{T}X^{T}y}&space;-&space;\boldsymbol{y^{T}y}&space;\right&space;)\right&space;\}&space;=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\frac{\partial&space;}{\partial&space;w}&space;\left&space;\{&space;\frac{1}{n_{train}}&space;\left&space;(&space;\boldsymbol{w^{T}X^{T}Xw}&space;-&space;\boldsymbol{y^{T}Xw}&space;-&space;\boldsymbol{w^{T}X^{T}y}&space;-&space;\boldsymbol{y^{T}y}&space;\right&space;)\right&space;\}&space;=0" title="\small \frac{\partial }{\partial w} \left \{ \frac{1}{n_{train}} \left ( \boldsymbol{w^{T}X^{T}Xw} - \boldsymbol{Y^{T}Xw} - \boldsymbol{w^{T}X^{T}Y} - \boldsymbol{Y^{T}Y} \right )\right \} =0" /></a>
  - （）の中の第２項と第３項は展開すれば同じもの。
  - 微分すると、
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\frac{1}{n_{train}}&space;\left&space;(&space;2\boldsymbol{X^{T}Xw}&space;-&space;2\boldsymbol{X^{T}y}&space;\right&space;)=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\frac{1}{n_{train}}&space;\left&space;(&space;2\boldsymbol{X^{T}Xw}&space;-&space;2\boldsymbol{X^{T}y}&space;\right&space;)=0" title="\small \frac{1}{n_{train}} \left ( 2\boldsymbol{X^{T}Xw} - 2\boldsymbol{X^{T}y} \right )=0" /></a>
  - これには以下を利用（Aが行列、wが横ベクトル）
    - ∂(w^{T}Aw)/∂w = (A^{T} +A)w
    - ∂(Aw)/∂w = Aw
    - matrix cookbookを参照しても良い。
  - 解は
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\boldsymbol{\hat{w}}&space;=\left&space;(&space;\boldsymbol{X^{T}X}&space;\right&space;)^{-1}\boldsymbol{X^{T}y}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\boldsymbol{\hat{w}}&space;=\left&space;(&space;\boldsymbol{X^{T}X}&space;\right&space;)^{-1}\boldsymbol{X^{T}y}" title="\small \boldsymbol{\hat{w}} =\left ( \boldsymbol{X^{T}X} \right )^{-1}\boldsymbol{X^{T}y}" /></a>
  - yの予測値は以下になる。
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\boldsymbol{\hat{y}}&space;=\boldsymbol{X}&space;\left&space;(&space;\boldsymbol{X^{T}X}&space;\right&space;)^{-1}\boldsymbol{X^{T}y}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\boldsymbol{\hat{y}}&space;=\boldsymbol{X}&space;\left&space;(&space;\boldsymbol{X^{T}X}&space;\right&space;)^{-1}\boldsymbol{X^{T}y}" title="\small \boldsymbol{\hat{y}} =\boldsymbol{X} \left ( \boldsymbol{X^{T}X} \right )^{-1}\boldsymbol{X^{T}y}" /></a>
  - <a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{200}&space;\small&space;\boldsymbol{X}&space;\left&space;(&space;\boldsymbol{X^{T}X}&space;\right&space;)^{-1}\boldsymbol{X^{T}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{200}&space;\small&space;\boldsymbol{X}&space;\left&space;(&space;\boldsymbol{X^{T}X}&space;\right&space;)^{-1}\boldsymbol{X^{T}}" title="\small \boldsymbol{X} \left ( \boldsymbol{X^{T}X} \right )^{-1}\boldsymbol{X^{T}}" /></a>の部分は真の値yから予測値へ射映させる行列という意味で射影行列という。
  - 逆行列が常に存在しない問題について
    - 存在する条件は？
      - ムーアペンローズの一般化逆行列
    - 存在しない時はどうするか
      - 逆行列っぽいものを持ってくる
- データを動かす。
  - DLも含めて機械学習は外装問題にとても弱いと考えた方が良い。