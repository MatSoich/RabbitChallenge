<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
サポートベクトルマシン
============
# 概要

- SVMとは
  - SVMは、2クラス分類問題の代表的な手法の1つで、未知データに対して高い予測精度を持つ
  - SVMはもともと2クラス分類問題のために考案されたが、その後、回帰問題や教師なし問題などへも応用されている。
  - 今回記述するSVMのことを、他の問題用のSVMと区別して、特にサポートベクトル(SV)分類(support vector classi cation)と呼ぶ
- 決定関数と分類境界
  - 一般に2クラス分類問題では、特徴ベクトルxがどちらのクラスに属するか判定するために次の決定関数(decision function)と呼ばれる関数f(x)が使われます。
  > \\\(f(x) =\boldsymbol w^T \boldsymbol x+b\\\)
  - ここでwは、特徴ベクトルxと同じ次元の数値ベクトルで、\\\(w^Tx\\\)は次のように計算されます。
  > \\\(\boldsymbol w^T \boldsymbol x=(w_1,\cdots,w_n)\left(\begin{array}{c} x_1 \\x_2 \\\vdots \\x_n\end{array}
\right)
=w_1x_1 + w_2x_2 + \cdots + w_nx_n=\sum_{i+1}^n w_ix_i\\\)
  - ある入力データxに対して決定関数f(x)を計算し、その符号により2つのクラスに分類します。
  > \\\(\begin{eqnarray} \max ( a, b ) = \begin{cases}a & (a \geqq b)\\b & (a \lt b) \end{cases}\end{eqnarray}

y= sgn f(x) =8>><>>:+1  (f(x)>0)1  (f(x)<0)\\\)
  - f(x) = 0が、特徴ベクトル x= (x1; x2)Tを2つのクラスに分ける境界線になっていることが分かります。一般にこのような境界を分類境界(classi cation boundary)と言います
- 線形サポートベクトル分類(ハードマージン)
  - SVMに基づいてこのn個の訓練データからパラメータw、bを決定する方法を説明していきます
  - まずは分離可能な場合を考えることにします。
  - yif(xi)>0  (i= 1;2;; n)(5)もし正しく分類できていないデータがある場合には、そのデータ(xi; yi)に対してsgnf(xi)とyiの符号が逆になっているためyif(xi)<0となります。
  - 分類境界を挟んで2つのクラスがどのくらい離れているかをマージン(margin)と呼びます。マージン最大化(margin maximization)させる。
  - マージンを最大化することは、分類境界f(x) = 0と分類境界から最も近くにあるデータxiとの距離を最大化すること
  - のf(x) = 0とxiの距離は、次式で表されます。jf(xi)j∥w∥=jwTxi+bj∥w∥(6)ここで、∥w∥はwのL2ノルム∥w∥=√w21+w22++w2n(7)を表します。式(6)は、いわゆる「点と平面の距離の公式」です
  - 分類境界f(x) = 0と分類境界から最も近くにあるデータとの距離は、次のように表現することができます。
  > \\\(\displaystyle \min_{i} \frac{y_i[\boldsymbol w^t \boldsymbol x_i + b]}{||\boldsymbol w||} = \frac{1}{|| \boldsymbol w||} \min_i [y_i[\boldsymbol w^t \boldsymbol x_i + b] = \frac{M(\boldsymbol w, b)}{||\boldsymbol w||}\\\)
  - ージンを最大化することは、この量を最大化することと等価ですから、SVMの目的関数は次のように書ける
  - w,bを規格化して簡潔に表して、
  > aaaaa
  - このような分離可能性を仮定したSV分類のことを一般にハードマージンと呼ぶ。
- 線形サポートベクトル分類(ソフトマージン）
  - 仮定をなくし、SV分類を分離可能でないデータに適用できるように拡張していきます。このようなタスクは、前のセクションのハードマージンに対して、ソフトマージン(soft margin)と呼ばれます。
  - yi[wTxi+b]1i(i= 1;; n)
  - \\\(\slack i\\\)はマージン内に入るデータや誤分類されたデータに対する誤差を表す変数となっており、スラック変数(slack variable)と呼ばれます。
  - w;b;[12∥w∥2+Cn∑i=1i];   yi[wTxi+b]1i;   i0  (i= 1;; n)
- SVMにおける双対表現
  - 上記のハードマージン、ソフトマージンの最適化問題のことを、SV分類の主問題(primalproblem)と呼ぶ。この主問題を解けば分類境界を決定できますが、SV分類ではこの主問題と等価な双対問題(dual problem)18の方を解くことが一般的となっています。
    - 主問題と比べて双対問題の方が変数を少なくできる
    - 分類境界の非線形化20を考える上で双対問題の形式(双対形式)の方が有利となる
- 双対問題の導出
  - 
- 主問題と双対問題の関係
- カーネルを用いた非線形分離への拡張




# 実践
[リンク先に記載](https://github.com/MatSoich/RabbitChallenge/blob/master/機械学習/codes/7.サポートベクトルマシン.ipynb)
or
[ダウンロード](codes/7.サポートベクトルマシン.ipynb)