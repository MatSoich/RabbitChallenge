<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


機械学習
============

# ロジスティック回帰モデル

- 各クラスに当てはまる確率を直接予測する。p（C_k|x)を直接モデル化
  - 識別的アプローチ(SVMなどは識別の関数の構成、という形をとることもある。)
  - 一方で生成的アプローチというものを存在する
  - \\\(p(C_k)\\\)と\\\(p(x\|C_k)\\\)をモデル化し、その後Bayesの定理を用いる。
    - \\\(\displaystyle p(C_k\|x)= \frac{p(C_k,x)}{p(x)} = \frac{p(x,C_k)}{p(x)} = \frac{p(x\|C_k)p(C_k)}{p(x)}\\\) 
    - 生成的アプローチは幅を持って推定できるのと、新たなデータを生み出せる可能性があるので、生成的アプローチを実施している。
  - ロジスティック関数は状態空間から[0, 1]に確率を潰している。
  - 識別的アプローチは確率を与えてくれるので、確率が0.8以上で１、0.2以下で0と判定する、それ以外は判定を保留する、というようなことができる。SVMなどの識別モデルではこういったことはできない（難しい）。
  - 生成的アプローチに関して、ここで尤度関数を考える。尤度関数は、ある分布（例えばベルヌーイ分布）を想定して、データを固定し、パラメータを変化させて、尤度関数を最大化するようなパラメータを選択する。これを最尤推定という。
    - 機械学習においては、既知の過去の試行であるデータを元に最適なパラメータ(例えば確率p)を推定するので、最尤推定の枠組みに収まると考える。最小二乗法も最尤法の中の一つである。
  - ロジスティック関数の最尤推定の話
    - sigmoid関数の微分はsigmoid関数で表せる。
    - \\\( \displaystyle \frac{\partial \sigma (x)} {\partial x} = \frac{\partial } {\partial x} \left \lbrace \frac{1}{1 + \exp(-ax)^2} \right \rbrace =a\sigma(x)\left ( 1 - \sigma(x) \right )\\\)
    - ロジスティック回帰の分布についてはベルヌーイ分布として、以下のように表せる。
    - \\\( \displaystyle P(y_{1},y_{2},\cdot \cdot \cdot, y_{n} \|w_{1},w_{2},\cdot \cdot \cdot, w_{m}) = \prod _{i=1}^{n} p_{i}^{y_{i}} (1-p_{i})^{(1-y_{i})}\\\)
    - p_iはsigmoid関数（logistic関数）なので
    - \\\(\displaystyle = \prod _{i=1}^{n} \sigma(\boldsymbol{w^T x_{i}})^{y_{i}} (1-\sigma(\boldsymbol{w^T x_{i}}))^{(1-y_{i})}\\\)
    - これをパラメータwの式とすると、尤度関数L(w)として考えることができる。
    - この時最適なパラメータwを考える。
  - 対数尤度関数を作る意味
    - １つは掛け算から足し算にすることで計算が簡単になる。
    - もう一つ（実装上においては最も）重要なのは、確率の掛け算による桁落ちを防ぐ。
  - 損失関数として対数尤度をとった計算は以下。
    - \\\(E(\boldsymbol{w}) = - log(L(\boldsymbol{w})) = - \displaystyle\sum_{i=1}^{n} \left \{ y_{i}log(p_{i}) + (1 - y_{i})log(1 - p_{i}) \right \}\\\)
    - ここで
    - \\\(\displaystyle p_{i} = \sigma(\boldsymbol{w}^{T}\boldsymbol{x}_{i}) = \frac{1}{1+exp(\boldsymbol{w}^{T}\boldsymbol{x}_{i})}\\\)
    - \\\(\displaystyle z_{i} = \boldsymbol{w}^{T}\boldsymbol{x}_{i}\\\)
    - とする。
    - 損失関数の一回微分が0となる値を求める。
    - \\\(\displaystyle \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol{w}} = -\sum_{i=1}^{n} \frac{\partial E_{i}}{\partial p_{i}}\frac{\partial p_{i}}{\partial z_{i}}\frac{\partial z_{i}}{\partial \boldsymbol{w}} = \sum_{i=1}^{n}(\frac{y_{i}}{p_{i}} - \frac{1 - y_{i}}{1 - p_{i}})p_{i}(1 - p_{i})\boldsymbol{x}_{i} = \sum_{i=1}^{n}(y_{i} - p_{i})\boldsymbol{x}_{i}\\\)
    - 勾配降下法では、この値を基準にしてパラメータを更新していく。etaは調整項
    - \\\(\displaystyle \boldsymbol{w^{k+1}} = \boldsymbol{w^{k}} + \eta \sum_{i=1}^{n}(y_{i}-p_{i})\boldsymbol{x_{i}}\\\)
    - 確率的勾配降下法
    - \\\(\displaystyle \boldsymbol{w^{k+1}} = \boldsymbol{w^{k}} + \eta (y_{i}-p_{i})\boldsymbol{x_{i}}\\\)
    - データを一つずつランダムに選んでパラメータを更新していく。
    - 更新を繰り返して、解を探索することができる。

