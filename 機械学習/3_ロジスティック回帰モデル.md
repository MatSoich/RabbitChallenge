<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


機械学習
============

# ロジスティック回帰モデル

- 各クラスに当てはまる確率を直接予測する。\\\(p（C_k\|x)\\\)を直接モデル化
  - 識別的アプローチ(SVMなどは識別の関数の構成、という形をとることもある。)
  - 一方で生成的アプローチというものも存在する
  - \\\(p(C_k)\\\)と\\\(p(x\|C_k)\\\)をモデル化し、その後Bayesの定理を用いる。
    > \\\(\displaystyle p(C_k\|x)= \frac{p(C_k,x)}{p(x)} = \frac{p(x,C_k)}{p(x)} = \frac{p(x\|C_k)p(C_k)}{p(x)}\\\) 
    - 生成的アプローチは幅を持って推定できるのと、新たなデータを生み出せる可能性があるので、生成的アプローチを実施している。
  - ロジスティック関数は状態空間から[0, 1]に確率を潰している。
  - 識別的アプローチは確率を与えてくれるので、確率が0.8以上で１、0.2以下で0と判定する、それ以外は判定を保留する、というようなことができる。SVMなどの識別モデルではこういったことはできない（難しい）。
  - 生成的アプローチに関して、ここで尤度関数を考える。尤度関数は、ある分布（例えばベルヌーイ分布）を想定して、データを固定し、パラメータを変化させて、尤度関数を最大化するようなパラメータを選択する。これを最尤推定という。
    - 機械学習においては、既知の過去の試行であるデータを元に最適なパラメータ(例えば確率p)を推定するので、最尤推定の枠組みに収まると考える。最小二乗法も最尤法の中の一つである。
- ロジスティック関数の最尤推定の話
  - sigmoid関数の微分はsigmoid関数で表せる。
  > \\\( \displaystyle \frac{\partial \sigma (x)} {\partial x} = \frac{\partial } {\partial x} \left \lbrace \frac{1}{1 + \exp(-ax)^2} \right \rbrace =a\sigma(x)\left ( 1 - \sigma(x) \right )\\\)
  - ロジスティック回帰の分布についてはベルヌーイ分布として、以下のように表せる。
  > \\\( \displaystyle P(y_1,y_2,\cdot \cdot \cdot, y_n \|w_1,w_2,\cdot \cdot \cdot, w_m) = \prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{(1-y_i)}\\\)
  - p_iはsigmoid関数（logistic関数）なので
  > \\\(\displaystyle = \prod_{i=1}^{n} \sigma(\boldsymbol{w^T x_i})^{y_i} (1-\sigma(\boldsymbol{w^T x_i}))^{(1-y_i)}\\\)
  - これをパラメータwの式とすると、尤度関数L(w)として考えることができる。
  - この時最適なパラメータwを考える。
- 対数尤度関数を作る意味
  - １つは掛け算から足し算にすることで計算が簡単になる。
  - もう一つ（実装上においては最も）重要なのは、確率の掛け算による桁落ちを防ぐ。
- 損失関数として対数尤度をとった計算は以下。
  > \\\(E(\boldsymbol{w}) = - log(L(\boldsymbol{w})) = - \displaystyle\sum_{i=1}^n \left \lbrace y_ilog(p_i) + (1 - y_i)log(1 - p_i) \right \rbrace\\\)
- ここで
  > \\\(\displaystyle p_{i} = \sigma(\boldsymbol w^T \boldsymbol x_{i}) = \frac{1}{1+\exp(\boldsymbol w^T \boldsymbol x_i)}\\\)
  > \\\(\displaystyle z_{i} = \boldsymbol{w}^{T}\boldsymbol{x}_{i}\\\)
- とする。
- 損失関数の一回微分が0となる値を求める。
  > \\\(\displaystyle \frac{\partial E(\boldsymbol w)}{\partial \boldsymbol w} = -\sum_{i=1}^n \frac{\partial E_i}{\partial p_i}\frac{\partial p_i}{\partial z_i}\frac{\partial z_i}{\partial \boldsymbol w} = \sum_{i=1}^n(\frac{y_i}{p_i} - \frac{1 - y_i}{1 - p_i})p_i(1 - p_i)\boldsymbol x_i = \sum_{i=1}^n(y_i - p_i)\boldsymbol x_i\\\)
- 勾配降下法では、この値を基準にしてパラメータを更新していく。
  > \\\(\displaystyle \boldsymbol w^{k+1} = \boldsymbol w^k + \eta \sum_{i=1}^n(y_i-p_i)\boldsymbol x_i \\\)
  - (\\\(\eta\\\)は調整項)
- 確率的勾配降下法では以下のような式になる。
  > \\\(\displaystyle \boldsymbol w^{k+1} = \boldsymbol{w^k} + \eta (y_i-p_i)\boldsymbol{x_i}\\\)
  - 確率的勾配降下法ではデータを一つずつランダムに選んでパラメータを更新される。
  - 更新を繰り返して、解を探索することができる。

# 実践
- タイタニックデータを使い、ロジスティック回帰を実施する。
- Fareのみの一変数で作成
```python
#運賃だけのリストを作成
data1 = titanic_df.loc[:, ["Fare"]].values
#生死フラグのみのリストを作成
label1 =  titanic_df.loc[:,["Survived"]].values
from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
model.fit(data1, label1)
model.predict([[61]])

X_test_value = model.decision_function(data1) 
print (model.intercept_)
print (model.coef_)
```
- 切片及び係数は以下
> [-0.94131796]
> [[0.01519666]]
- Fare及びPclass_Genderという合成変数で作成。
```python
titanic_df['Gender'] = titanic_df['Sex'].map({'female': 0, 'male': 1}).astype(int)
titanic_df['Pclass_Gender'] = titanic_df['Pclass'] + titanic_df['Gender']
#運賃とジェンダーのルールを作成
data2 = titanic_df.loc[:, ["AgeFill", "Pclass_Gender"]].values
#生死フラグのみのリストを作成
label2 = titanic_df.loc[:,["Survived"]].values
model2 = LogisticRegression()
model2.fit(data2, label2)
```
- 切片及び係数は以下
> [3.42539711]
> [[-0.00221662 -1.33656736]]

```python
from sklearn.model_selection import train_test_split
traindata1, testdata1, trainlabel1, testlabel1 = train_test_split(data1, label1, test_size=0.2)
traindata1.shape
trainlabel1.shape
traindata2, testdata2, trainlabel2, testlabel2 = train_test_split(data2, label2, test_size=0.2)
traindata2.shape
trainlabel2.shape
#本来は同じデータセットを分割しなければいけない。(簡易的に別々に分割している。)
eval_model1=LogisticRegression()
eval_model2=LogisticRegression()
predictor_eval1=eval_model1.fit(traindata1, trainlabel1).predict(testdata1)
predictor_eval2=eval_model2.fit(traindata2, trainlabel2).predict(testdata2)

from sklearn import metrics
print(metrics.classification_report(testlabel1, predictor_eval1))
print(metrics.classification_report(testlabel2, predictor_eval2))
```
- 一変数での Precision値、recall値、F1scoreは以下。
>               precision    recall  f1-score   support
> 
>            0       0.63      0.94      0.75       106
>            1       0.70      0.19      0.30        73
> 
>     accuracy                           0.64       179
>    macro avg       0.66      0.57      0.53       179
> weighted avg       0.66      0.64      0.57       179

- 二変数での Precision値、recall値、F1scoreは以下。
>               precision    recall  f1-score   support
> 
>            0       0.81      0.85      0.83       123
>            1       0.63      0.55      0.59        56
> 
>     accuracy                           0.76       179
>    macro avg       0.72      0.70      0.71       179
> weighted avg       0.75      0.76      0.76       179
- F1-ScoreはRecall(再現率)とPrecision(適合率)はトレードオフの関係にありこれらの調和平均をとったもの。F1-scoreの値からに変数での方が正確な予測ができていることがわかる。
- 次に混同行列を一変数、二変数で作成して詳細を確認する。

```python
from sklearn.metrics import confusion_matrix
confusion_matrix1=confusion_matrix(testlabel1, predictor_eval1)
confusion_matrix2=confusion_matrix(testlabel2, predictor_eval2)
confusion_matrix1

```
- 一変数
> array([[100,   6],
>        [ 59,  14]])

```python
confusion_matrix2

```
- 二変数
> array([[105,  18],
>        [ 25,  31]])      

- 上記より一変数では、左下のFalse Neagativeつまり本当は死亡しているのに生存していると予測してしまっている場合が多いことがわかる。


[リンク先に記載](https://github.com/MatSoich/RabbitChallenge/blob/master/機械学習/codes/3.ロジスティック回帰モデル.ipynb)
or
[ダウンロード](codes/3.ロジスティック回帰モデル.ipynb)