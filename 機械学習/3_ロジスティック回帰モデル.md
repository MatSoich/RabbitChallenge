<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


機械学習
============

# ロジスティック回帰モデル

- 各クラスに当てはまる確率を直接予測する。\\\(p（C_k\|x)\\\)を直接モデル化
  - 識別的アプローチ(SVMなどは識別の関数の構成、という形をとることもある。)
  - 一方で生成的アプローチというものを存在する
  - \\\(p(C_k)\\\)と\\\(p(x\|C_k)\\\)をモデル化し、その後Bayesの定理を用いる。
    - \\\(\displaystyle p(C_k\|x)= \frac{p(C_k,x)}{p(x)} = \frac{p(x,C_k)}{p(x)} = \frac{p(x\|C_k)p(C_k)}{p(x)}\\\) 
    - 生成的アプローチは幅を持って推定できるのと、新たなデータを生み出せる可能性があるので、生成的アプローチを実施している。
  - ロジスティック関数は状態空間から[0, 1]に確率を潰している。
  - 識別的アプローチは確率を与えてくれるので、確率が0.8以上で１、0.2以下で0と判定する、それ以外は判定を保留する、というようなことができる。SVMなどの識別モデルではこういったことはできない（難しい）。
  - 生成的アプローチに関して、ここで尤度関数を考える。尤度関数は、ある分布（例えばベルヌーイ分布）を想定して、データを固定し、パラメータを変化させて、尤度関数を最大化するようなパラメータを選択する。これを最尤推定という。
    - 機械学習においては、既知の過去の試行であるデータを元に最適なパラメータ(例えば確率p)を推定するので、最尤推定の枠組みに収まると考える。最小二乗法も最尤法の中の一つである。
  - ロジスティック関数の最尤推定の話
    - sigmoid関数の微分はsigmoid関数で表せる。
    - \\\( \displaystyle \frac{\partial \sigma (x)} {\partial x} = \frac{\partial } {\partial x} \left \lbrace \frac{1}{1 + \exp(-ax)^2} \right \rbrace =a\sigma(x)\left ( 1 - \sigma(x) \right )\\\)
    - ロジスティック回帰の分布についてはベルヌーイ分布として、以下のように表せる。
    - \\\( \displaystyle P(y_1,y_2,\cdot \cdot \cdot, y_n \|w_1,w_2,\cdot \cdot \cdot, w_m) = \prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{(1-y_i)}\\\)
    - p_iはsigmoid関数（logistic関数）なので
    - \\\(\displaystyle = \prod_{i=1}^{n} \sigma(\boldsymbol{w^T x_i})^{y_i} (1-\sigma(\boldsymbol{w^T x_i}))^{(1-y_i)}\\\)
    - これをパラメータwの式とすると、尤度関数L(w)として考えることができる。
    - この時最適なパラメータwを考える。
  - 対数尤度関数を作る意味
    - １つは掛け算から足し算にすることで計算が簡単になる。
    - もう一つ（実装上においては最も）重要なのは、確率の掛け算による桁落ちを防ぐ。
  - 損失関数として対数尤度をとった計算は以下。
    - \\\(E(\boldsymbol{w}) = - log(L(\boldsymbol{w})) = - \displaystyle\sum_{i=1}^n \left \lbrace y_ilog(p_i) + (1 - y_i)log(1 - p_i) \right \rbrace\\\)
    - ここで
    - \\\(\displaystyle p_{i} = \sigma(\boldsymbol{w}^T \boldsymbol{x}_{i}) = \frac{1}{1+\exp(\boldsymbol{w}^{T} \boldsymbol{x}_{i})}\\\)
    - \\\(\displaystyle z_{i} = \boldsymbol{w}^{T}\boldsymbol{x}_{i}\\\)
    - とする。
    - 損失関数の一回微分が0となる値を求める。
    - \\\(\displaystyle \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol{w}} = -\sum_{i=1}^n \frac{\partial E_i}{\partial p_i}\frac{\partial p_i}{\partial z_i}\frac{\partial z_i}{\partial \boldsymbol{w}} = \sum_{i=1}^n(\frac{y_i}{p_i} - \frac{1 - y_i}{1 - p_i})p_i(1 - p_i)\boldsymbol{x}_i = \sum_{i=1}^n(y_i - p_i)\boldsymbol{x}_i\\\)
    - 勾配降下法では、この値を基準にしてパラメータを更新していく。etaは調整項
    - \\\(\displaystyle \boldsymbol{w^{k+1}} = \boldsymbol{w^k} + \eta \sum_{i=1}^n(y_i-p_i)\boldsymbol{x_i}\\\)
    - 確率的勾配降下法
    - \\\(\displaystyle \boldsymbol{w^{k+1}} = \boldsymbol{w^k} + \eta (y_i-p_i)\boldsymbol{x_i}\\\)
    - データを一つずつランダムに選んでパラメータを更新していく。
    - 更新を繰り返して、解を探索することができる。

