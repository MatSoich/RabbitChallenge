<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

# 中間層と出力層の活性化関数の違い
- 中間層のニューラルネットワークは次の層の入力として適切なもの
- 出力層のニューラルネットワークは求める値を示すもの

# 出力層
- 出力層より計算された予測値が出力される。
- 推測値と正解の隔たりを表す関数を誤差関数という。
  - 自身があるものとないものとで誤差関数の値が違うように表すことができる（ものもある）
  - 単純な誤差関数には２乗和誤差がある。
    - 微分の計算を行いやすくするために1/2が前についている。
  - 実際には分類問題の場合においてはクロスエントロピー誤差を用いる場合が多い。

# 出力層における活性化関数の意味
- 中間層...値の強弱の調整
- 出力層...各値の比率は変えずに問題の出力に合わせる形に変換
# 出力における各活性化関数の使い分け

| | 回帰 | 二値文類 |多クラス分類|
|:---:|:---:|:---:|:---:|
|**活性化関数**|恒等写像<br>\\\(f(u)=u\\\) |シグモイド関数<br>\\\(\displaystyle f(u)=\frac{1}{1+e^{-u}}\\\)|ソフトマックス関数<br>\\\(\displaystyle f(\boldsymbol i,\boldsymbol u)=\frac{e^{u_i}}{\sum_{k=1}^Ke^{u_k}}\\\)|
|**誤差関数**|二乗誤差|交差エントロピー|交差エントロピー|

- シグモイド関数の式
```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```
- 平均二乗誤差の式
```python
def mean_squared_error(d, y):
    return np.mean(np.square(d - y)) / 2
```
# 確認テスト7
- 上表内のソフトマックス関数の式の①左辺②右辺分子③右辺分母が下記Python式のどこにあるかを示し、１行ずつ処理の説明をせよ

- ソフトマックス関数の式
```python
def softmax(x): #①
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T
    x = x - np.max(x) # オーバーフロー対策
    return np.exp(x) / np.sum(np.exp(x))# 分子が②で分母が③　
```
# 確認テスト８
- 交差エントロピーの式\\\(\displaystyle E_n(w) = \sum_{i=1}^ld_i\log y_i\\\)の①左辺②右辺が下記Python式のどこにあるかを示し、１行ずつ処理の説明をせよ

- 交差エントロピーの式
```python
def cross_entropy_error(d, y):#①定義部分、関数の出力にあたる。
    if y.ndim == 1:
        d = d.reshape(1, d.size)
        y = y.reshape(1, y.size)
        
    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換
    if d.size == y.size:
        d = d.argmax(axis=1)
             
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size #②のところ
```
# 実装は入力層〜中間層のと部分の実装、及び誤差逆伝播法の実装と合わせて実装することにする。

