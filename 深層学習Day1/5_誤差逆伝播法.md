<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


# 誤差逆伝播法
- \\\(\nabla E\\\)をどのように導くか。
- M層のニューラルネットワークであればM箇所に重みが出現
  > \\\(\displaystyle \nabla E = \frac{\partial E}{\partial \boldsymbol w} = \left[ \frac{\partial E}{\partial w_1} \cdot \frac{\partial E}{\partial w_2} \cdots\frac{\partial E}{\partial w_m} \right]\\\)
- 方法１ - 数値微分
  - プログラムで微小な数値を生成し、擬似的に微分を計算する一般的な手法
  > \\\(\displaystyle \frac{\partial E}{\partial w_m} \approx \frac{E(w_m +h)-E(w_m-h)}{2h} \\\)
  - デメリットとして計算量が非常に多くなる。
    - 各wそれぞれについての計算をm回繰り返すため
- 方法２ - 誤差逆伝播法
  - それぞれの場所で計算される重みを結果から逆順に計算することによって再利用する。

# 確認テスト12
- 誤差逆伝播法では不要な再起処理を避けるため、既に行った計算過程を保持しているソースコードを1-3コードより示せ。
```python
    # 出力層でのデルタ
    delta2 = functions.d_mean_squared_error(d, y)
    # b2の勾配
    grad['b2'] = np.sum(delta2, axis=0)
    # W2の勾配
    grad['W2'] = np.dot(z1.T, delta2)
    # 中間層でのデルタ
    #delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)

    ## 試してみよう
    delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1)

    delta1 = delta1[np.newaxis, :]
    # b1の勾配
    grad['b1'] = np.sum(delta1, axis=0)
    x = x[np.newaxis, :]
    # W1の勾配
    grad['W1'] = np.dot(x.T, delta1)
```
# 誤差逆伝播法の数式上の処理