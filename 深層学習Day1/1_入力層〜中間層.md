<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

入力層〜中間層
============

# 識別器における生成モデル/識別モデル/識別関数
- 生成モデルはデータのクラス条件付き密度を求めている。
- 計算量が多いので、単に分類結果をえたいだけなら分類結果を得たいだけなら、直接データクラスに属する確率を求める識別モデルで良い。
- 識別モデル（確率的識別モデル）と識別関数（決定的識別モデル）
- 識別モデルは確率を出すので、モデルの信頼度を出すことができる。識別関数ではそれがわからない。
# ニューラルネットワークの全体像
- 入力層、中間層、出力層に大きく分かれる。
- 重みとバイアスを使ってそれぞれの層で変換を行う。
- ディープラーニングは何をしているか？
  - プログラムを自分で複雑に書く代わりにデータを入力することでプログラムを自力で調整するメカニズム
# 確認テスト１
- 次のネットワークを紙にかけ
  - 入力層：２ノード１層
  - 中間層：３ノード２層
  - 出力層：１ノード１層
![kakunin](imgs/kakunin1.png)

# NNにできること
- 分類と回帰
- ４つ以上ニューラルネットワークを持つものは深層ニューラルネットワークと呼ばれる。
- 実用例：自動売買、チャットボット、音声解釈などなど
- 数字にできないものは工夫して数字に置き換えることでニューラルネットワークを利用することができる。

# 入力層〜中間層
- 入力層
  - 何かしらの数字を入力する部分（入力x）
  - 各入力の重要度を表す重みwを用いて数値を変換する。
  - 各出力は以下のようになる
  - \\\(\boldsymbol u = w_1x_1 + w_2x_2 + \dots + b = \boldsymbol W\boldsymbol x + \boldsymbol b \\\)
  - wやbがニューラルネットで学習させて変化することになる。
  - \\\(\boldsymbol u\\\)は総入力でこれに活性化関数を通すことで結果が得られる。

# 確認テスト２
- 入力層から中間層の図式に動物分類の例を当てはめてみる。
![kakunin](imgs/kakunin2.png)
# 確認テスト３
- \\\(\boldsymbol u = w_1x_1 + w_2x_2 + \dots + b = \boldsymbol W\boldsymbol x + \boldsymbol b \\\)をPythonで記載。
> u = np.dot(W, x) + b
> 
# 確認テスト４ 
[1_1_forward_propagation.ipynb](codes/1_1_forward_propagation.ipynb)の順伝播（3層・複数ユニット）より中間層の出力を定義しているソースを抜き出す。
> \# 2層の総入力
> u2 = np.dot(z1, W2) + b2
> \# 2層の総出力
> z2 = functions.relu(u2)