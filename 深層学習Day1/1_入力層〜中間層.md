<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

入力層〜中間層
============

# 識別器における生成モデル/識別モデル/識別関数
- 生成モデルはデータのクラス条件付き密度を求めている。
- 計算量が多いので、単に分類結果をえたいだけなら分類結果を得たいだけなら、直接データクラスに属する確率を求める識別モデルで良い。
- 識別モデル（確率的識別モデル）と識別関数（決定的識別モデル）
- 識別モデルは確率を出すので、モデルの信頼度を出すことができる。識別関数ではそれがわからない。
# ニューラルネットワークの全体像
- 入力層、中間層、出力層に大きく分かれる。
- 重みとバイアスを使ってそれぞれの層で変換を行う。
- ディープラーニングは何をしているか？
  - プログラムを自分で複雑に書く代わりにデータを入力することでプログラムを自力で調整するメカニズム
# 確認テスト１
- 次のネットワークを紙にかけ
  - 入力層：２ノード１層
  - 中間層：３ノード２層
  - 出力層：１ノード１層
![kakunin](imgs/kakunin1.png)

# NNにできること
- 分類と回帰
- ４つ以上ニューラルネットワークを持つものは深層ニューラルネットワークと呼ばれる。
- 実用例：自動売買、チャットボット、音声解釈などなど
- 数字にできないものは工夫して数字に置き換えることでニューラルネットワークを利用することができる。

# 入力層〜中間層
- 入力層
  - 何かしらの数字を入力する部分（入力x）
  - 各入力の重要度を表す重みwを用いて数値を変換する。
  - 各出力は以下のようになる
  - \\\(\boldsymbol u = w_1x_1 + w_2x_2 + \dots + b = \boldsymbol W\boldsymbol x + \boldsymbol b \\\)
  - wやbがニューラルネットで学習させて変化することになる。
  - \\\(\boldsymbol u\\\)は総入力でこれに活性化関数を通すことで結果が得られる。

# 確認テスト２
- 入力層から中間層の図式に動物分類の例を当てはめてみる。
![kakunin](imgs/kakunin2.png)
# 確認テスト３
- \\\(\boldsymbol u = w_1x_1 + w_2x_2 + \dots + b = \boldsymbol W\boldsymbol x + \boldsymbol b \\\)をPythonで記載。
> u = np.dot(W, x) + b
> 
# 確認テスト４ 
[1_1_forward_propagation.ipynb](codes/1_1_forward_propagation.ipynb)の順伝播（3層・複数ユニット）より中間層の出力を定義しているソースを抜き出す。
> \# 2層の総入力
> u2 = np.dot(z1, W2) + b2
> \# 2層の総出力
> z2 = functions.relu(u2)

# 実装演習
- 入力層3、隠れ層5、出力層4のニューラルネットワークを作成する。

```python
# ネットワーク作成用の関数
def init_network():
    network = {}
    #_各パラメータのshapeを表示    
    input_layer_size = 3
    hidden_layer_size= 5
    output_layer_size = 4
    #_ネットワークのウエイトをランダム生成
    network['W1'] = np.random.rand(input_layer_size, hidden_layer_size)
    network['W2'] = np.random.rand(hidden_layer_size,output_layer_size)
    #_ネットワークのバイアスをランダム生成
    network['b1'] =  np.random.rand(hidden_layer_size)
    network['b2'] =  np.random.rand(output_layer_size)
    # 確認用の出力
    print_vec("重み1", network['W1'] )
    print_vec("重み2", network['W2'] )
    print_vec("バイアス1", network['b1'] )
    print_vec("バイアス2", network['b2'] )

    return network

# 上記ネットワークを元に順伝播の関数を作成。
def forward(network, x):
    W1, W2 = network['W1'], network['W2']
    b1, b2 = network['b1'], network['b2']
    # 1層の総入力
    u1 = np.dot(x, W1) + b1
    # 1層の総出力
    z1 = functions.relu(u1)
    # 2層の総入力
    u2 = np.dot(z1, W2) + b2
    # 出力値
    y = functions.softmax(u2)
    # 出力の確認。
    print_vec("総入力1", u1)
    print_vec("中間層出力1", z1)
    print_vec("総入力2", u2)
    print_vec("出力1", y)
    print("出力合計: " + str(np.sum(y)))
        
    return y, z1


# 入力値（３つのインプット）
x = np.array([1., 2.,  3.])

# 目標出力(4つのアウトプット)
d = np.array([0, 0, 0, 1])

# ネットワークの初期化
network =  init_network()

# ニューラルネットワークのアウトプット出力
y, z1 = forward(network, x)

# クロスエントロピー誤差計算
loss = functions.cross_entropy_error(d, y)

## 表示
print("\n##### 結果表示 #####")
print_vec("出力", y)
print_vec("訓練データ", d)
print_vec("交差エントロピー誤差",  loss)

```

- 出力は以下のようになった。

> *** 重み1 ***
> [[0.82170159 0.69423859 0.32978581 0.80963016 0.63349563]
>  [0.303549   0.0483753  0.48834144 0.14278324 0.73894736]
>  [0.97807563 0.32094065 0.25591134 0.61541109 0.92212197]]
> shape: (3, 5)
> 
> *** 重み2 ***
> [[0.24863542 0.8769228  0.24340611 0.82396951]
>  [0.85398749 0.90928363 0.80615185 0.84388483]
>  [0.2422982  0.49443733 0.65472751 0.61487176]
>  [0.03586267 0.62239113 0.53038497 0.22515723]
>  [0.73841218 0.01617789 0.97770196 0.12256726]]
> shape: (5, 4)
> 
> *** バイアス1 ***
> [0.38976826 0.4170236  0.05199498 0.45549633 0.09089713]
> shape: (5,)
> 
> *** バイアス2 ***
> [0.47536492 0.90916441 0.18436067 0.28082877]
> shape: (4,)
> 
> ##### 順伝播開始 #####
> *** 総入力1 ***
> [4.75279476 2.17083474 2.12619768 3.39692624 4.96865338]
> shape: (5,)
> 
> *** 中間層出力1 ***
> [4.75279476 2.17083474 2.12619768 3.39692624 4.96865338]
> shape: (5,)
> 
> *** 総入力2 ***
> [ 7.81685463 10.2967736  11.14286327  8.71009687]
> shape: (4,)
> 
> *** 出力1 ***
> [0.02314258 0.27632932 0.64398989 0.05653821]
> shape: (4,)
> 
> 出力合計: 1.0
> 
> ##### 結果表示 #####
> *** 出力 ***
> [0.02314258 0.27632932 0.64398989 0.05653821]
> shape: (4,)
> 
> *** 訓練データ ***
> [0 0 0 1]
> shape: (4,)
> 
> *** 交差エントロピー誤差 ***
> 2.872836881649026
> shape: ()

- 結果を見ると、訓練データでは1となっている４つの目の値が0.05程度と非常に小さくなってしまい、正確な予測となっていない。そのため、交差エントロピー誤差が非常に高くなってしまっている。
- 現状ではうまく予測できていないことがわかる。